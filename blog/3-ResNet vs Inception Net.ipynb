{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ResNet\n",
    "#### 一. ResNet\n",
    "1. Residual Block - 残差块    \n",
    "  1. 参差的意思为, 由于深度神经网络中存在梯度消失或梯度爆炸现象. 在计算${ a }^{ [l+2] }=g({ z }^{ [l+2] })$时往往得不到正确结果    \n",
    "  2. 此时计算${ a }^{ [l+2] }=g({ z }^{ [l+2] }+{ a }^{ [l] })$的方法, 加入残差项${ a }^{ [l] }$    \n",
    "<img src='../img/residualblock1.png' height='80%' width='80%'>\n",
    "\n",
    "2. Residual Network  \n",
    " 参差网络为使用残差块构成的网络\n",
    " <img src='../img/residualnetwork.png' height='70%' width='70%'>\n",
    "\n",
    "####  二. ResNet的良好表现\n",
    "1. ResNet的表现  \n",
    "  1. ResNet可以训练很深的神经网络 (往往达到100多层), 而避免梯度消失和梯度爆炸问题\n",
    "  2. ResNet随着网络层数的增多, 其有更良好的表现. 而不会像传统网络一样, 网络层数增多到一定程度带来负面效果\n",
    "3. 为什么参差网络可以构建超深的网络  \n",
    " 由于${ a }^{ [l+2] }=g({ z }^{ [l+2] }+{ a }^{ [l] })$, 因此即使"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inception network\n",
    "\n",
    "#### 一. 1\\*1 filter矩阵  \n",
    "1. 1\\*1 宽度和高度的filter矩阵, 可以保持输出矩阵和输入矩阵有相同的宽度和高度  \n",
    "2.  根据filter个数的不同, 1\\*1\\*n的filter可以减小输入矩阵的#channels数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二. Inception Net\n",
    "1. 当我们不知道该使用1\\*1,还是3\\*3,还是5\\*5的卷积核(filter). 或者是否使用pooling运算. 此时可选用Inception网络, 按这些运算都进行一遍.  并将结果在channel上连接起来  \n",
    "  1. 输入张量为28\\*28\\*192. 分别使用1\\*1,3\\*3,5\\*5和MaxPooling的卷积核进行运算.\n",
    "  2. 不同卷积核使用不同的padding填充, 使得产生的张量在宽度和高度上一致. 只是#channels不同. \n",
    "  3. 将这些不同的张量在channel维度上拼接起来\n",
    "  <img src='../img/inveptionnet1.png' height='50%' width='50%'>\n",
    "\n",
    "2. 计算成本  \n",
    "  1. 使用5\\*5的卷积核举例. 如下图, 该卷积核把输入张量28\\*28\\*192,经过5\\*5的卷积后, 输出张量28\\*28\\*32  \n",
    "    即要得到输出28\\*28\\*32个数, 得到每个数要乘5\\*5\\*192, 所以共给算28\\*28\\*32\\*5\\*5\\*192=120M次  \n",
    "  2. 现在改进计算方法, 在中间加入1\\*1的卷积核, 先输出 28\\*28\\*16的\"瓶颈层\",, 再用瓶颈层计算最终输出.  \n",
    "    共计算: 28\\*28\\*16\\*192+28\\*28\\*32\\*5\\*5\\*16 = 12.4M\n",
    "  <img src='../img/computationdecrease1.png' height='95%' width='95%'>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 卷积网络的小技巧\n",
    "\n",
    "#### 一. 迁移学习\n",
    "1. 比如要训练一个识别tigger跟cat和两者都不是的分类网络. 但我们手上只有很少的tigger和cat的图片. 如果从头建立CNN网络则有样本不足的问题  \n",
    " 因此, 可以直接下载imagenet的开源图片分类的神经网络, 并冻结前面所有隐藏层的参数, (同时下载网络结构和参数)只训练最后一层-softmax层进行分类  \n",
    "2. 如果有较多的tigger和cat图片, 可以少冻结前面的基层隐藏层, 而训练后面的几层参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
