{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sklearn 进行 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/gw/g9jgl36s1q7bjqm73zz5q4y80000gn/T/jieba.cache\n",
      "Loading model cost 0.409 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################1. 加入自定义词频,使其不易分割##########################\n",
    "import jieba\n",
    "\n",
    "jieba.suggest_freq('沙瑞金',True)\n",
    "jieba.suggest_freq('易学习',True)\n",
    "jieba.suggest_freq('王大路',True)\n",
    "jieba.suggest_freq('京州', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################2. 对文档分词并将结果写入文件##########################\n",
    "# 第一个文档分词\n",
    "with open('../自然语言处理/data/nlp_test0.txt') as f :\n",
    "    doc1 = f.read()\n",
    "    doc_decode = doc1.decode('utf-8') # utf-8\n",
    "    doc_cut = jieba.cut(doc_decode)\n",
    "    res = ' '.join(doc_cut).encode('utf-8')\n",
    "    with open('../自然语言处理/data/nlp_test1.txt','w') as f2:\n",
    "        f2.write(res)\n",
    "# 第二个文档分词 \n",
    "with open('../自然语言处理/data/nlp_test2.txt') as f :\n",
    "    doc1 = f.read()\n",
    "    doc_decode = doc1.decode('utf-8') # utf-8\n",
    "    doc_cut = jieba.cut(doc_decode)\n",
    "    res = ' '.join(doc_cut).encode('utf-8')\n",
    "    with open('../自然语言处理/data/nlp_test3.txt','w') as f2:\n",
    "        f2.write(res)\n",
    "# 第三个文档分词\n",
    "with open('../自然语言处理/data/nlp_test4.txt') as f :\n",
    "    doc1 = f.read()\n",
    "    doc_decode = doc1.decode('utf-8') # utf-8\n",
    "    doc_cut = jieba.cut(doc_decode)\n",
    "    res = ' '.join(doc_cut).encode('utf-8')\n",
    "    with open('../自然语言处理/data/nlp_test5.txt','w') as f2:\n",
    "        f2.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \n",
      "\n",
      "沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。 \n",
      "\n",
      "347 年 （ 永和 三年 ） 三月 ， 桓 温兵 至 彭模 （ 今 四川 彭山 东南 ） ， 留下 参军 周楚 、 孙盛 看守 辎重 ， 自己 亲率 步兵 直攻 成都 。 同月 ， 成汉 将领 李福 袭击 彭模 ， 结果 被 孙盛 等 人 击退 ； 而桓温 三 战三胜 ， 一直 逼近 成都 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################3. 读入分好词的数据文件到内存 ##########################\n",
    "with open('../自然语言处理/data/nlp_test1.txt') as f3:\n",
    "    res1 = f3.read()\n",
    "print res1\n",
    "with open('../自然语言处理/data/nlp_test3.txt') as f4:\n",
    "    res2 = f4.read()\n",
    "print res2\n",
    "with open('../自然语言处理/data/nlp_test5.txt') as f5:\n",
    "    res3 = f5.read()\n",
    "print res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################4. 导入停用词表 ##########################\n",
    "stpword_path = '../自然语言处理/data/stop_words.txt'\n",
    "with open(stpword_path,'rb') as f:\n",
    "    stw_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3, 98)\n",
      "  (0, 43)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 97)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 44)\t2\n",
      "  (0, 83)\t2\n",
      "  (0, 86)\t1\n",
      "  (0, 92)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 70)\t5\n",
      "  (0, 26)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 85)\t1\n",
      "  (0, 33)\t1\n",
      "  :\t:\n",
      "  (2, 82)\t1\n",
      "  (2, 59)\t1\n",
      "  (2, 45)\t1\n",
      "  (2, 51)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 52)\t2\n",
      "  (2, 73)\t1\n",
      "  (2, 62)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 89)\t1\n",
      "  (2, 75)\t1\n",
      "  (2, 41)\t2\n",
      "  (2, 32)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 71)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 48)\t1\n",
      "  (2, 34)\t1\n",
      "  (2, 49)\t2\n",
      "  (2, 69)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 64)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 81)\t1\n"
     ]
    }
   ],
   "source": [
    "##########################5. 统计文档词频(加入停用词) ##########################\n",
    "corpus = [res1,res2,res3]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countVector = CountVectorizer(stop_words=stw_list)\n",
    "wordfreq_corpus = countVector.fit_transform(corpus)\n",
    "\n",
    "print type(wordfreq_corpus)\n",
    "print wordfreq_corpus.shape # (3,98)\n",
    "print wordfreq_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00942172 0.99057828]\n",
      " [0.01526749 0.98473251]\n",
      " [0.98400318 0.01599682]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lj/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/lj/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "##########################6. LDA主题分布 ##########################\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=2,learning_offset=50,random_state=0)\n",
    "doc_res = lda.fit_transform(wordfreq_corpus)\n",
    "\n",
    "print doc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 98)\n",
      "[[1.32808385 1.24916841 0.88691155 0.72653726 0.77035094 0.80927654\n",
      "  1.26951152 1.23362076 0.80375809 0.80118309 0.86893767 1.21611832\n",
      "  0.64385751 0.81772975 0.91910765 0.69693257 0.88609681 1.30999765\n",
      "  0.74259872 0.67175512 0.85343906 1.29454946 0.71655002 0.71931202\n",
      "  0.74561057 0.74896993 0.68792474 1.21566452 0.73949298 1.24801295\n",
      "  0.78110131 0.79565213 1.19407724 0.75302842 1.18454712 0.68329188\n",
      "  0.68892633 0.801672   0.82359435 0.78897586 0.80580121 1.65191491\n",
      "  0.76398663 0.77452388 0.71447041 1.2772968  0.80797068 0.6923886\n",
      "  1.29363005 1.67787145 0.75096055 1.29570495 1.70859634 1.26599767\n",
      "  0.80629255 0.80536285 0.70492437 0.85605351 0.74964622 1.18773741\n",
      "  0.91221309 0.84481098 1.21297799 0.85372325 1.26120828 0.84198265\n",
      "  0.79343956 0.72486239 0.79609094 1.30210297 0.80465331 1.33282142\n",
      "  0.73160525 1.25652926 0.73837264 1.26459512 0.79519933 1.17070004\n",
      "  1.23582458 0.81920528 0.72407136 1.30993005 1.22351999 0.89396026\n",
      "  0.76687931 0.73692527 0.71190982 0.74936095 0.75787593 1.33798169\n",
      "  0.71876478 0.818743   0.71907373 1.14833728 0.82559547 0.7893829\n",
      "  0.71719281 0.81050794]\n",
      " [0.72670025 0.73285683 1.65992531 1.61133105 1.71741376 1.12803968\n",
      "  0.71169008 0.7721544  1.61303555 1.22779072 1.24026649 0.7602173\n",
      "  1.3515778  1.28947443 1.18897734 1.21244779 1.72063793 0.80510813\n",
      "  2.13151985 1.186642   1.22688851 0.7637499  1.27588929 1.68429546\n",
      "  1.17484019 1.20258877 1.3570734  0.76715676 1.64843558 0.79567885\n",
      "  1.22224593 1.23662793 0.69620159 1.19813086 0.74082083 1.24838049\n",
      "  1.16504347 1.72558934 1.23609288 1.25153329 1.2053953  0.75539721\n",
      "  1.28011977 1.28494538 1.71290006 0.73861673 1.25178263 1.26391037\n",
      "  0.68803584 0.80498689 1.19275823 0.76619297 0.67247832 0.77941942\n",
      "  1.64307422 1.21815391 1.33811761 3.06574527 1.25181382 0.7633679\n",
      "  2.01428571 1.78288481 0.73291354 2.19886927 0.71469169 2.19832383\n",
      "  1.19043317 1.1893182  1.34984022 0.71379176 4.41234117 0.80094555\n",
      "  1.20341469 0.78944144 1.21833731 0.73326966 1.15430595 0.70014094\n",
      "  0.8076733  1.20349281 1.21771096 1.73383723 0.74886069 2.1300477\n",
      "  1.22097428 1.21894404 1.2401388  1.17752113 1.15254262 0.75218477\n",
      "  1.17763946 1.27647751 1.26801987 0.70837047 1.13297309 1.24887452\n",
      "  1.28502643 1.19935743]]\n"
     ]
    }
   ],
   "source": [
    "## 打印主题下的词分布\n",
    "print lda.components_.shape  # (2,98)\n",
    "print lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
