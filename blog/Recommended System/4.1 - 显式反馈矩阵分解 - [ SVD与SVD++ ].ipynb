{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 矩阵分解\n",
    "\n",
    "#### 一. 何为矩阵分解\n",
    "1. 在前面的紧邻推荐中, 我们找到了基于用户和基于物品的紧邻推荐. 但是, 近邻推荐有2方面问题  \n",
    "  1. 物品之间的相关性, 不随物品向量维度的增加而增加\n",
    "  2. 矩阵稀疏, 计算结果不稳定\n",
    "  \n",
    "2. 如果说前面的近邻推荐有很强的解释性, name矩阵分解则是更强的机器学习类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SVD算法\n",
    "#### 一. 基础SVD算法\n",
    "1. 推荐系统中的SVD算法, 不是线性代数意义上的SVD. 而是一个机器学习问题  \n",
    "2. SVD把`用户-物品评分矩阵`分解为2个小矩阵的乘积.   \n",
    " 若有$m$个用户, $n$个物品,评分矩阵维度$m*n$.把评分矩阵拆解为用户矩阵和物品矩阵的乘积,且用户和物品向量的维度都是$k$.即$${ A }_{ m*n }\\approx { U }_{ m*k }{ V }_{ n*k }^{ T }$$\n",
    "3. 若我们通过矩阵分解, 得到了用户和物品的矩阵, 则用户u和物品i的向量表示分别为$p_u$和$q_i$, 把物品i推荐给用户u的推荐分数为$$\\hat { { r }_{ ui } } ={ p }_{ u }*{ q }_{ i }^{ T }$$\n",
    "1. 基础SVD算法的损失函数为最小化平方误差加上正则项:$$L=\\sum _{ u,i }^{  }{ { \\left( { r }_{ ui }-{ p }_{ u }*{ q }_{ i }^{ T } \\right)  }^{ 2 }+\\lambda \\left( { \\left\\| { p }_{ u } \\right\\|  }^{ 2 }+{ \\left\\| { q }_{ i } \\right\\|  }^{ 2 } \\right)  } $$初始化$p,q$矩阵后, 使用梯度下降计算计算矩阵中元素的值  \n",
    "2. 接下来, 我们会给出基础SVD的优化方向, 这些都是损失函数的不同, 但解决办法都是通过梯度下降求解\n",
    "\n",
    "#### 二. 增加偏置信息的SVD\n",
    "1. 每个用户打分的严格程度不同, 因此, 在评分中,加上用户相对物品全局平均评分的偏置项. 所以评分变成$$\\hat { { r }_{ ui } } =\\mu +{ b }_{ i }+{ b }_{ u }+{ p }_{ u }*{ q }_{ i }^{ T }$$  \n",
    "  1. $\\mu $ : 物品全局平均分\n",
    "  2. ${ b }_{ i }$ : 物品偏执\n",
    "  3. ${ b }_{ u }$ : 用户偏执\n",
    "  4. ${ p }_{ u }*{ q }_{ i }^{ T }$ : 用户向量\\*物品向量\n",
    "  \n",
    "2. 添加偏置的SVD损失函数为 : $$L=\\sum _{ u,i }^{  }{ { \\left( { r }_{ ui }-\\mu -{ b }_{ i }-{ b }_{ u }-{ p }_{ u }*{ q }_{ i } \\right)  }^{ 2 } } +\\lambda \\left( { \\left\\| { p }_{ u } \\right\\|  }^{ 2 }+{ \\left\\| { q }_{ i } \\right\\|  }^{ 2 }+{ { b }_{ i } }^{ 2 }+{ { b }_{ u } }^{ 2 } \\right) $$因此, 带偏置的SVD损失函数需要多学习2个参数:用户偏执和物品偏执\n",
    "\n",
    "#### 三. 增加隐式反馈的SVD++\n",
    "1. 何为隐式反馈\n",
    "  1. 用户对物品的评分数据一般会很少, 因为大多数用户只是操作了物品, 但最后没有给出评分.评分数据就是显示反馈   \n",
    "    相对的, 隐式反馈指用户操作过但没评分的物品. 这个数据很多, 为了在SVD中加入隐士反馈, 就有了SVD++算法  \n",
    "  2. 上面介绍的基础SVD, 把评分拆解成用户矩阵和物品矩阵的乘积. 每个用户对应的用户向量和每个物品对应的物品向量, 都称为\"隐因子向量\"  \n",
    "    如果我们能够学习到用户操作过的物品(隐式反馈)的隐因子向量, 并加入SVD算法中, 就有了SVD++算法.  \n",
    "    其次, 如果我们再加上用户属性代表的隐因子向量, 则在冷启动推荐中, 会有更好的效果\n",
    "2. SVD++计算方法\n",
    "  1. 设用户的隐式反馈物品为$x_i$, 用户每个属性对应的隐因子向量为$y_a$. 则带隐式反馈和用户属性的对物品i的评分为 : $$\\hat { { r }_{ ui } } =\\mu +{ b }_{ i }+{ b }_{ u }+\\left( { p }_{ u }+\\frac { \\sum _{ i\\in N(u) }^{  }{ { x }_{ i } }  }{ \\sqrt { { \\left\\| N(u) \\right\\|  }^{ 2 } }  } +\\sum _{ a\\in A(u) }^{  }{ { y }_{ a } }  \\right) *{ q }_{ i }^{ T }$$  \n",
    "    1. $N(u)$ : 用户操作过的物品\n",
    "    2. $A(u)$ : 用户的属性 \n",
    "  2. 相比带偏置的SVD算法, SVD++又需要多学习2个参数 : '隐式反馈的物品'隐因子向量和'用户属性'对应的隐因子向量\n",
    "\n",
    "#### 四. 带时间参数的SVD++\n",
    "1. 因为人们的口味会随着时间变化, 因此可以再SVD中考虑加入时间因素  \n",
    "  1. 对评分按照时间加权，让久远的评分更趋近平均值；\n",
    "  2. 对评分时间划分区间，不同的时间区间内分别学习出隐因子向量，使用时按照区间使用对应的隐因子向量来计算；\n",
    "  3. 对特殊的期间，如节日、周末等训练对应的隐因子向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 隐式反馈矩阵分解\n",
    "1. 何为隐式反馈矩阵  \n",
    " 上面的SVD矩阵分解, 主要是从评分预测的角度进行矩阵填充. 预测用户会对物品打多少分  \n",
    " 但由于评分数据太少了, 所以真实情况往往是直接对用户的隐式反馈矩阵进行分解. \n",
    "2. 隐式反馈二分类问题   \n",
    "  1. 隐式反馈矩阵由0,1组成. 代表用户是否操作过这个物品. 因此, 用户对物品的预测变成了2分类预测, 即预测用户是否会对该物品产生兴趣  \n",
    "  2. 虽然隐式反馈可以抽象成2分类问题, 但要注意, 我们的数据只能表示用户对这些物品感兴趣, 却不能表示用户对未操作过的物品明确地表示不感兴趣. 因此, 这是一个`one-class`问题.\n",
    "  3. 如何选取负例来训练二分类问题  \n",
    "   可以优先选择热度大且用户未作出反应的物品作为负例. 因为我们认为, 热度越大, 用户越可能直到这个物品的存在, 但仍是没做出反应, 则说明用户很可能对这个物品不感兴趣.  \n",
    "  4. 二分类问题的损失函数  \n",
    "   因为用户做出的物品表示为1, 但是这些物品也有权重大小之分. 因为有的物品的浏览/操作每个物品的次数不同. 为了加入这种浏览物品次数影响, 产生了加权的损失函数 : $$L=\\sum _{ u,i }^{  }{ { c }_{ ui }{ \\left( { r }_{ ui }-{ p }_{ u }*{ q }_{ i }^{ T } \\right)  }^{ 2 } } +\\lambda \\left( { \\left\\| { p }_{ u } \\right\\|  }^{ 2 }+{ \\left\\| { q }_{ i } \\right\\|  }^{ 2 } \\right) $$  \n",
    "     1. ${ c }_{ ui }$ : 用户u对物品i的权重, 一般设置为${ c }_{ ui }=1+\\alpha c$. 其中$\\alpha $为超参数,经验设置为40.而$c$表示用户操作物品的次数\n",
    "3. 如何产生推荐结果  \n",
    " 表面上, 通过上述隐式反馈矩阵分解后得到了用户和物品的隐因子向量. 直接点积即可获得. 但由于用户和物品的个数众多, 全部计算还是很费时间. 因此, 优良中优化方法 :  \n",
    "   1. 采用特殊的数据结构(Ball-Tree)存储物品的隐因子向量.   \n",
    "    这样给出一个用户的隐因子向量, 即可立即从数据结构中找出最相似的k个物品.  \n",
    "    开源实现有: Facebook 给出了自己的开源实现 Faiss，类似的开源实现还有 Annoy，KGraph，NMSLIB。\n",
    "   2. 或者可以先对物品做聚类, 找出聚类中心. 看用户和哪个聚类中心点积后得分高, 从而把选择物品变成选择类簇, 再从类簇中选择商品进行推荐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
