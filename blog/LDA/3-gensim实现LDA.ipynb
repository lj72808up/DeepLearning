{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gensim进行LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import os\n",
    "from lxml import etree\n",
    "import lxml\n",
    "import jieba\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. 原始文本处理\n",
    "1. 搜狗新闻文件是xml文件, 我们只抽取content标签中的文字. 形如  \n",
    "  ```xml\n",
    "  <docs>\n",
    "    <doc>\n",
    "        <contenttitle>新股发行＂减速＂本周拟融资额下降逾４成</contenttitle>\n",
    "        <content>对于新股扩容是否影响股市涨跌的讨论近期不绝于耳</content>\n",
    "    </doc>\n",
    "  </docs>\n",
    "  ```\n",
    "2. 预料中都是全角,应转换成半角\n",
    "2. 文件中包含不可见字符和英文字母, 简单过滤掉  \n",
    "    ```python\n",
    "    re.sub('[\\u3000,\\ue40c]','',text)\n",
    "    re.sub('[a-zA-Z0-9]+','',text)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "\n",
    "# 解析单个xml文件中的content\n",
    "def getContentFromFile(_filepath,x):\n",
    "    '''_filepath: 待解析的搜狗新闻文本,xml格式\n",
    "        x: 解析出来的文本加到x后面, 作为语料库\n",
    "    '''\n",
    "    parser = etree.XMLParser(encoding='utf-8',huge_tree=True) #XML解析器\n",
    "    text = open(_filepath).read()\n",
    "    root = etree.fromstring(text,parser=parser)\n",
    "    docs = root.findall('doc') # <doc>元素\n",
    "    for doc in docs:\n",
    "        if type(doc) is lxml.etree._Element:\n",
    "            for child_elem in doc.getchildren():\n",
    "                if (child_elem.tag=='content'): # or (child_elem.tag=='contenttitle')\n",
    "                    text = child_elem.text\n",
    "                    if (text is not None) and (type(text) is str) and (text != ''):\n",
    "                        text = re.sub('[\\u3000,\\ue40c]','',text)\n",
    "                        text = strQ2B(text)\n",
    "                        text = re.sub('[a-zA-Z0-9]+','',text)\n",
    "                        x.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 22978 documents\n"
     ]
    }
   ],
   "source": [
    "# 解析文件夹下的所有xml文件\n",
    "data_dir = '/home/lj/data/sogou_new2012'\n",
    "x_origion = []\n",
    "for subfile in os.listdir(data_dir):\n",
    "    subfile_path = os.path.join(data_dir,subfile)\n",
    "    getContentFromFile(subfile_path,x_origion)\n",
    "print('corpus has %s documents' % len(x_origion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. 对过滤后的文本进行分词, 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/lj/ideaprojects/DeepLearning/data/dict.txt.small ...\n",
      "Loading model from cache /tmp/jieba.ube6b3622f3ed3770dfb4b3dbec1af2a2.cache\n",
      "Loading model cost 0.210 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.set_dictionary('../../data/dict.txt.small')\n",
    "x_cut = [] #分词后,去除停用词的语料库\n",
    "\n",
    "# 获取停用词列表\n",
    "with open('../../data/stop_words_utf8.txt') as f:\n",
    "    content = f.readlines()\n",
    "    stopwords = [w.strip() for w in content]\n",
    "\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "for doc in x_origion:\n",
    "    content = list(jieba.cut(doc))\n",
    "    x_cut.append([w for w in content if w not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三, 从分好词的文本中训练LDA\n",
    "1. gensim可以自动从预料中学习dictionary, 免除手动创建的繁琐  \n",
    "2. gensim统计文本出现的单词词频,生成带词频的BOW模型. 格式如下 :   \n",
    " ```python\n",
    "  [(word1_index,freq1),(word2_index,freq2) ... ]\n",
    " ```\n",
    "3. 使用gensim的`LdaModel训练`, 需要制定词典,语料库,主体数量\n",
    "4. 获取每个主题下的词分布  \n",
    "  ```python\n",
    "  lda.print_topics(20)\n",
    "  ```\n",
    "5. 获取某个文章下的主题分布  \n",
    "  ```python\n",
    "  lda[document]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "dictionary = Dictionary(x_cut)  # 从语料库中获取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1片文档形成的(index,count): \n",
      " [(0, 4), (1, 4), (2, 1), (3, 1), (4, 1), (5, 5), (6, 10), (7, 1), (8, 2), (9, 1)] ..\n"
     ]
    }
   ],
   "source": [
    "# gensim根据词典,统计文本中出现的单词index和单词出现次数, 形成[(index,count)]形式的列表\n",
    "index_count_list = dictionary.doc2bow(x_cut[0])\n",
    "print('第1片文档形成的(index,count): \\n',index_count_list[:10],'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有分词后的文档, 生成如上面所示的[(index,count)]列表\n",
    "corpus = [dictionary.doc2bow(doc) for doc in x_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda模型\n",
    "lda = LdaMulticore(corpus=corpus,  # LDA训练语料\n",
    "               id2word=dictionary, # id到单词的映射表\n",
    "               num_topics=20)      # LDA主题数量 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.090*\"容量\" + 0.077*\"系列\" + 0.048*\"英寸\" + 0.044*\"主频\" + 0.042*\"尺寸\" + 0.042*\"硬盘\" + 0.041*\"屏幕\" + 0.040*\"内存\" + 0.039*\"芯片\" + 0.039*\"显卡\"'),\n",
       " (1,\n",
       "  '0.007*\"月\" + 0.007*\"支持\" + 0.005*\"市场\" + 0.005*\"年\" + 0.005*\"公司\" + 0.005*\"中国\" + 0.005*\"产品\" + 0.004*\"功能\" + 0.004*\"经济\" + 0.004*\"汽车\"'),\n",
       " (2,\n",
       "  '0.009*\"座椅\" + 0.007*\"市场\" + 0.006*\"调节\" + 0.006*\"电动\" + 0.004*\"公司\" + 0.004*\"系统\" + 0.004*\"年\" + 0.004*\"后\" + 0.004*\"月\" + 0.004*\"中国\"'),\n",
       " (3,\n",
       "  '0.009*\"电话\" + 0.008*\"地址\" + 0.007*\"联系\" + 0.007*\"店铺\" + 0.006*\"手机\" + 0.005*\"号码\" + 0.005*\"座椅\" + 0.005*\"市场\" + 0.005*\"公司\" + 0.004*\"月\"'),\n",
       " (4,\n",
       "  '0.009*\"市场\" + 0.006*\"月\" + 0.004*\"产品\" + 0.004*\"公司\" + 0.004*\"经济\" + 0.004*\"中国\" + 0.004*\"年\" + 0.003*\"企业\" + 0.003*\".%\" + 0.003*\"表示\"'),\n",
       " (5,\n",
       "  '0.008*\"座椅\" + 0.006*\"调节\" + 0.005*\"类型\" + 0.005*\"年\" + 0.004*\"市场\" + 0.004*\"电动\" + 0.004*\"方向盘\" + 0.004*\"容量\" + 0.004*\"后\" + 0.004*\"系统\"'),\n",
       " (6,\n",
       "  '0.046*\"容量\" + 0.026*\"硬盘\" + 0.026*\"系列\" + 0.025*\"内存\" + 0.025*\"主频\" + 0.025*\"英寸\" + 0.024*\"芯片\" + 0.024*\"屏幕\" + 0.022*\"尺寸\" + 0.022*\"产品\"'),\n",
       " (7,\n",
       "  '0.006*\"公司\" + 0.004*\"市场\" + 0.004*\"中\" + 0.004*\"类型\" + 0.004*\"座椅\" + 0.004*\"接口\" + 0.004*\"年\" + 0.004*\"经济\" + 0.003*\"后\" + 0.003*\"月\"'),\n",
       " (8,\n",
       "  '0.015*\"支持\" + 0.006*\"产品\" + 0.006*\"手机\" + 0.005*\"市场\" + 0.004*\"类型\" + 0.004*\"容量\" + 0.004*\"月\" + 0.004*\"座椅\" + 0.004*\"调节\" + 0.004*\"功能\"'),\n",
       " (9,\n",
       "  '0.007*\"公司\" + 0.007*\"市场\" + 0.006*\"系列\" + 0.006*\"产品\" + 0.006*\".%\" + 0.005*\"月\" + 0.005*\"价格\" + 0.005*\"华硕\" + 0.004*\"容量\" + 0.004*\"年\"'),\n",
       " (10,\n",
       "  '0.016*\"容量\" + 0.012*\"座椅\" + 0.011*\"尺寸\" + 0.009*\"显卡\" + 0.008*\"屏幕\" + 0.008*\"华硕\" + 0.008*\"主频\" + 0.007*\"英寸\" + 0.007*\"产品\" + 0.007*\"硬盘\"'),\n",
       " (11,\n",
       "  '0.006*\"市场\" + 0.006*\"中\" + 0.004*\"政策\" + 0.004*\"企业\" + 0.004*\"中国\" + 0.004*\"月\" + 0.004*\"座椅\" + 0.003*\"公司\" + 0.003*\"后\" + 0.003*\"调节\"'),\n",
       " (12,\n",
       "  '0.071*\"系列\" + 0.054*\"华硕\" + 0.054*\"容量\" + 0.040*\"尺寸\" + 0.039*\"屏幕\" + 0.033*\"型号\" + 0.033*\"参数\" + 0.033*\"显卡\" + 0.031*\"芯片\" + 0.031*\"内存\"'),\n",
       " (13,\n",
       "  '0.006*\"中\" + 0.005*\"市场\" + 0.005*\"公司\" + 0.005*\"企业\" + 0.004*\"月\" + 0.004*\".%\" + 0.004*\"基金\" + 0.004*\"银行\" + 0.004*\"记者\" + 0.003*\"座椅\"'),\n",
       " (14,\n",
       "  '0.007*\"月\" + 0.005*\"支持\" + 0.005*\"产品\" + 0.005*\"市场\" + 0.004*\"英寸\" + 0.004*\"中国\" + 0.004*\"公司\" + 0.003*\"年\" + 0.003*\"尺寸\" + 0.003*\"经济\"'),\n",
       " (15,\n",
       "  '0.031*\"系列\" + 0.024*\"容量\" + 0.024*\"参数\" + 0.022*\"尺寸\" + 0.019*\"华硕\" + 0.019*\"产品\" + 0.018*\"内存\" + 0.018*\"英寸\" + 0.018*\"显卡\" + 0.017*\"芯片\"'),\n",
       " (16,\n",
       "  '0.008*\"座椅\" + 0.007*\"系统\" + 0.006*\"电动\" + 0.005*\"市场\" + 0.004*\"后排\" + 0.004*\"类型\" + 0.004*\"调节\" + 0.004*\"公司\" + 0.004*\"基金\" + 0.004*\"产品\"'),\n",
       " (17,\n",
       "  '0.006*\"公司\" + 0.005*\"市场\" + 0.005*\"支持\" + 0.005*\"中国\" + 0.004*\"座椅\" + 0.004*\"年\" + 0.004*\"手机\" + 0.004*\"经济\" + 0.004*\"类型\" + 0.004*\"中\"'),\n",
       " (18,\n",
       "  '0.009*\"市场\" + 0.007*\"公司\" + 0.006*\"年\" + 0.005*\"基金\" + 0.005*\"中\" + 0.005*\"月\" + 0.005*\"经济\" + 0.004*\".%\" + 0.004*\"增长\" + 0.004*\"投资\"'),\n",
       " (19,\n",
       "  '0.037*\"容量\" + 0.023*\"英寸\" + 0.022*\"尺寸\" + 0.021*\"屏幕\" + 0.021*\"显卡\" + 0.021*\"参数\" + 0.019*\"内存\" + 0.018*\"芯片\" + 0.018*\"硬盘\" + 0.018*\"主频\"')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取主题下词的分布\n",
    "lda.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.08417053),\n",
       " (2, 0.07729106),\n",
       " (6, 0.12837616),\n",
       " (10, 0.02020849),\n",
       " (11, 0.39385492),\n",
       " (14, 0.25584596),\n",
       " (15, 0.019009368)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第10篇文章的主体分布\n",
    "lda[corpus[11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有文章的主题分布形成ndarray\n",
    "m = len(corpus)\n",
    "docs_distribute = np.zeros((m,20))\n",
    "for i,doc in enumerate(corpus):\n",
    "    distribute = lda[doc] # 该文章的主体分布\n",
    "    for tupl in distribute:\n",
    "        docs_distribute[i,tupl[0]] = tupl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97060573, 0.02147073,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第1篇文章的主体分步\n",
    "docs_distribute[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
