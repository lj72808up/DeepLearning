{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一. 购物评价情感分析\n",
    "1. 本数据集包含两万多条中文标注语料，涉及六个领域的评论数据  \n",
    " 对这些评论数据先分词, 再倒入与训练好的词向量, 构建RNN模型进行情感分析\n",
    "<img src='img/zhongwenpingluing.jpg' height='60%' width='60%'>\n",
    "\n",
    "#### 二. 训练过程\n",
    "1. 读取文件, 构建标签\n",
    "2. 对文件内容分词, 使用gensim训练词向量, 并使用词向量中的单词创建词典\n",
    "3. 使用词典将文本内容转换成index\n",
    "4. 训练神经网络,保存模型和参数\n",
    "5. 加载模型文件,对新的评论作出预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一步:读取文件, 构建标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: (21105,) labels: (21105,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...\n",
       "1    作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...\n",
       "2    作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...\n",
       "3    作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...\n",
       "4    作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.加载数据文件\n",
    "neg_df = pd.read_excel('/home/lj/Downloads/neg.xls',header=None,index=None)\n",
    "pos_df = pd.read_excel('/home/lj/Downloads/pos.xls',header=None,index=None)\n",
    "\n",
    "corpus = pd.concat((pos_df[0],neg_df[0]))\n",
    "labels = np.concatenate((np.ones(pos_df.shape[0]),np.zeros(neg_df.shape[0])))\n",
    "\n",
    "print('corpus:',corpus.shape,'labels:',labels.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二步:分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.661 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [做, 父母, 一定, 刘墉, 心态, 不断, 学习, 不断, 进步, 不断, 补充, 新鲜...\n",
       "1    [作者, 真有, 英国人, 严谨, 风格, 提出, 观点, 进行, 论述, 论证, 物理学,...\n",
       "2    [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 计算结果, 支持, 其新,...\n",
       "3    [作者, 战, 之前, 拥抱, 令人, 叫绝, 日本, 战败, 美军, 占领, 没胡, 官僚...\n",
       "4    [作者, 少年, 时即, 喜, 阅读, 看出, 精读, 无数, 经典, 一个, 庞大, 内心...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.分词\n",
    "def strQ2B(ustring):\n",
    "    ''' ustring : 需要转换成半角的字符串 '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "\n",
    "def jieba_cut(_sentence):\n",
    "    semiangle_str = [strQ2B(w) for w in jieba.lcut(_sentence)]\n",
    "    return [w for w in semiangle_str if w not in stp_list]\n",
    "\n",
    "stp_list = open('../../data/stop_words_utf8.txt').read().splitlines()\n",
    "corpus_cut = corpus.apply(jieba_cut)\n",
    "\n",
    "corpus_cut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三步:gensim训练词向量, 转换文本文件为index, 构造嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim_dictionary中的元素为(index,word) [(2000, '众所周知'), (2001, '优'), (2002, '优于'), (2003, '优优'), (2004, '优先'), (2005, '优势'), (2006, '优化'), (2007, '优惠'), (2008, '优派'), (2009, '优点')]\n"
     ]
    }
   ],
   "source": [
    "# 3. gensim训练词嵌入,并构建词典\n",
    "gensim_model = word2vec.Word2Vec(corpus_cut,\n",
    "                                size=100,\n",
    "                                min_count=5,\n",
    "                                workers=8)\n",
    "gensim_model.save('Word2vec_model.pkl')\n",
    "\n",
    "all_tockens = gensim_model.wv.vocab.keys()  #gensim_model.wv.vocab为词典,{单词:<gensim.models.keyedvectors.Vocab>}\n",
    "gensim_dictionary = Dictionary()\n",
    "gensim_dictionary.doc2bow(all_tockens,allow_update=True)\n",
    "print('gensim_dictionary中的元素为(index,word):',list(gensim_dictionary.items())[2000:2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于转换文本为index和获取嵌入矩阵的中间结果\n",
    "word_index = {word:index+1 for index,word in gensim_dictionary.items()}  # 单词和index的映射\n",
    "word_vector = {word:gensim_model.wv.__getitem__(word) for word in word_index.keys()} # 单词和向量的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [2334, 9358, 747, 2919, 6184, 1185, 5081, 1185...\n",
       "1    [2159, 10022, 1390, 13332, 7156, 11558, 12450,...\n",
       "2    [2159, 12929, 2288, 11810, 6857, 5614, 7286, 1...\n",
       "3    [2159, 1538, 6937, 1941, 7671, 3316, 5885, 661...\n",
       "4    [2159, 5504, 4098, 13003, 9969, 10550, 7614, 1...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 文本内容转换为index\n",
    "def wordlist2indexlist(wordlist):\n",
    "    return [word_index[w] for w in wordlist if w in word_index]\n",
    "corpus_index = corpus_cut.apply(wordlist2indexlist)\n",
    "corpus_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建嵌入矩阵\n",
    "m = len(word_index) + 1\n",
    "n = 100\n",
    "embedding_matrix = np.zeros((m,n))\n",
    "for word,index in word_index.items():\n",
    "    embedding_matrix[index,:] = word_vector[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第四步:构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/devkit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1356700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 128)          117248    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,523,421\n",
      "Trainable params: 166,721\n",
      "Non-trainable params: 1,356,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Flatten,LSTM,Dense\n",
    "from keras import preprocessing\n",
    "from keras.layers.core import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# sentence length\n",
    "maxlen = 100\n",
    "corpus_index_pad = preprocessing.sequence.pad_sequences(corpus_index,maxlen=maxlen)\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_index_pad,labels,test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(m,n,input_length=maxlen))\n",
    "# model.add(Embedding(m,n,\n",
    "#                     mask_zero=True,\n",
    "#                     weights=[embedding_matrix],\n",
    "#                     input_length=maxlen))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False  # 冻结Embedding层\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16884 samples, validate on 4221 samples\n",
      "Epoch 1/10\n",
      "16884/16884 [==============================] - 157s 9ms/step - loss: 0.5217 - acc: 0.7453 - val_loss: 0.5718 - val_acc: 0.7517\n",
      "Epoch 2/10\n",
      "16884/16884 [==============================] - 157s 9ms/step - loss: 0.4456 - acc: 0.8037 - val_loss: 0.4332 - val_acc: 0.8107\n",
      "Epoch 3/10\n",
      "16884/16884 [==============================] - 159s 9ms/step - loss: 0.4105 - acc: 0.8237 - val_loss: 0.4497 - val_acc: 0.8017\n",
      "Epoch 4/10\n",
      "16884/16884 [==============================] - 158s 9ms/step - loss: 0.3871 - acc: 0.8348 - val_loss: 0.3950 - val_acc: 0.8332\n",
      "Epoch 5/10\n",
      "16884/16884 [==============================] - 160s 9ms/step - loss: 0.3677 - acc: 0.8418 - val_loss: 0.3859 - val_acc: 0.8375\n",
      "Epoch 6/10\n",
      "16884/16884 [==============================] - 160s 9ms/step - loss: 0.3498 - acc: 0.8529 - val_loss: 0.3757 - val_acc: 0.8387\n",
      "Epoch 7/10\n",
      "16884/16884 [==============================] - 157s 9ms/step - loss: 0.3341 - acc: 0.8586 - val_loss: 0.3613 - val_acc: 0.8420\n",
      "Epoch 8/10\n",
      "16884/16884 [==============================] - 156s 9ms/step - loss: 0.3130 - acc: 0.8683 - val_loss: 0.3596 - val_acc: 0.8515\n",
      "Epoch 9/10\n",
      "16884/16884 [==============================] - 158s 9ms/step - loss: 0.3022 - acc: 0.8724 - val_loss: 0.3603 - val_acc: 0.8500\n",
      "Epoch 10/10\n",
      "16884/16884 [==============================] - 159s 9ms/step - loss: 0.2816 - acc: 0.8830 - val_loss: 0.3664 - val_acc: 0.8579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7d00a3b38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第五步: 评价新的评论文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221/4221 [==============================] - 12s 3ms/step\n",
      "[0.3664279420972859, 0.8578535892533566]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test))\n",
    "model.save('model_chinese_comment.h5') #保存模型和权重"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
