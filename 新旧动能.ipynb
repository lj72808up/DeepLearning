{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 企查查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.按照红头文件匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    with open('data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "\n",
    "    \n",
    "def hadle_official_info():\n",
    "    '''讲地市的红头文件分词'''\n",
    "    df = pd.read_csv('./redhead.csv',delimiter='$',header=-1)\n",
    "    df.head()\n",
    "    df[1] = df[1].apply(lambda x:strQ2B(x))\n",
    "    jieba.set_dictionary('data/dict.txt.small')\n",
    "    x_cut = [] #分词后,去除停用词的语料库\n",
    "\n",
    "    # 获取停用词列表\n",
    "    stopwords = get_stop_words()\n",
    "    cut_res = df[1].apply(cut)\n",
    "    df[1] = cut_res\n",
    "    return df\n",
    "\n",
    "official_info_df = hadle_official_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_desc(\"我爱你中国,哈哈,呵呵,中国\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_info_df.to_csv('redhead2.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_info_df = pd.read_csv('redhead2.csv',index_col=0)\n",
    "official_info_df.index\n",
    "official_info_df.loc['青岛','1']\n",
    "type(official_info_df.loc['青岛','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_by_official(desc,location):\n",
    "    '''匹配红头文件, 存在则判定为新动能\n",
    "       -1 : 未找到红头文件\n",
    "       0: 匹配失败\n",
    "       1: 新动能匹配成功\n",
    "    '''\n",
    "    try:\n",
    "        print(location)\n",
    "        official_info_df.loc[location]\n",
    "    except KeyError: \n",
    "        return -1\n",
    "    official_info = official_info_df.loc[location,'1']\n",
    "    official_info = official_info.split('/')\n",
    "    crossinfo = [w for w in desc if w in official_info]\n",
    "    print('pattern ',location, ' ',len(crossinfo),'字符')\n",
    "    if len(crossinfo)>1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "desc = '青岛达能环保设备股份有限公司，成立于2006年10月，注册资金6300万元，占地面积15万平方米，生产建筑面积5.5万平方米，制造设备齐全，生产装备达到国内同行业领先水平，目前已具备年产值10亿元人民币的生产能力。青岛达能环保设备股份有限公司属锅炉辅机、环保设备制造行业。主要从事电站锅炉干式除渣系统产品、湿式除渣系统产品及烟气余热利用系统产品的研发、生产与销售。青岛达能环保设备股份有限公司为集研发、生产、销售为一体的国家高新技术企业、国家AAA级信用企业、山东省战略新兴产业重点企业、青岛企业技术中心、青岛民营科技创新先进单位。获得国家民营科技发展贡献奖、国家重点新产品、山东优秀节能成果奖等项目的荣誉称号和奖励。随着公司新业务的扩展，现面向社会诚招以下人才'\n",
    "desc = handle_desc(desc)\n",
    "location = '青岛'\n",
    "judge_by_official(desc,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LDA匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "qichacha_cut = pd.read_csv('qichacha_cut.csv',index_col=0)\n",
    "x_cut = qichacha_cut['经营范围'].apply(lambda x:x.split(',')).values\n",
    "print(x_cut)\n",
    "qichacha_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_excel(\"pos.xlsx\")\n",
    "pos_desc = pos_df[\"经营范围\"].dropna()\n",
    "pos_desc = pos_desc.apply(cut)\n",
    "pos_desc = pos_desc.apply(lambda x:x.split(\"/\"))\n",
    "pos_desc = pos_desc.values\n",
    "print(pos_desc.shape)\n",
    "print(x_cut.shape)\n",
    "y = np.concatenate((np.ones(pos_desc.shape[0]),np.zeros(x_cut.shape[0])),axis=0)\n",
    "x_cut = np.concatenate((pos_desc,x_cut),axis=0)\n",
    "x_cut.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLDA(topic_num):\n",
    "    from gensim.models import LdaMulticore\n",
    "    from gensim.corpora import Dictionary\n",
    "    # gensim\n",
    "    dictionary = Dictionary(x_cut)\n",
    "    print(dictionary)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in x_cut]\n",
    "    lda = LdaMulticore(corpus=corpus,  # LDA训练语料\n",
    "                   id2word=dictionary, # id到单词的映射表\n",
    "                   num_topics=topic_num)      # LDA主题数量 \n",
    "    return lda,corpus,dictionary\n",
    "topic_num = 5\n",
    "lda,corpus,dictionary = getLDA(topic_num)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribute(lda,doc):\n",
    "    return lda[doc]\n",
    "\n",
    "def get_all_doc_topic():\n",
    "    lda_matrix = np.zeros((x_cut.shape[0],topic_num),dtype='float64')\n",
    "    for index,doc in enumerate(corpus):\n",
    "        topics = get_topic_distribute(lda,corpus[index])\n",
    "        for topic_tuple in topics:\n",
    "            lda_matrix[index,topic_tuple[0]] = topic_tuple[1]\n",
    "    return lda_matrix\n",
    "\n",
    "print(get_topic_distribute(lda,corpus[0]))\n",
    "print(len(corpus))\n",
    "# 所有样本主题分布\n",
    "all_topic = get_all_doc_topic()\n",
    "# x_neg = all_topic[:1150]\n",
    "# x_pos = all_topic[1150:]\n",
    "# # svm正例\n",
    "# y_neg = np.zeros(x_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM训练\n",
    "def train_svm(lda_matrix,label):\n",
    "    clf = SVC()\n",
    "    clf.fit(lda_matrix, label) \n",
    "    return clf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_topic,y,test_size=0.1)\n",
    "clf = train_svm(x_train,y_train)\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#保存词典\n",
    "with open('dict_save', 'wb') as f:                     \n",
    "    picklestring = pickle.dump(dictionary, f)\n",
    "#保存lda模型\n",
    "lda.save('lda.model')\n",
    "#保存svm模型\n",
    "with open('svm.pickle', 'wb') as fw:\n",
    "    pickle.dump(clf, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(0, 0.2)\n",
      "(1, 0.2)\n",
      "(2, 0.2)\n",
      "(3, 0.2)\n",
      "(4, 0.2)\n",
      "[[0.2 0.2 0.2 0.2 0.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import jieba\n",
    "\n",
    "with open('/root/DeepLearning/dict_save', 'rb') as f:\n",
    "    load_dictionary = pickle.load(f)   # read file and build object\n",
    "from gensim import models\n",
    "load_lda = models.ldamodel.LdaModel.load('lda.model')\n",
    "# 加载svm.pickle\n",
    "with open('/root/DeepLearning/svm.pickle', 'rb') as fr:\n",
    "    load_clf = pickle.load(fr)\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    with open('/root/DeepLearning/data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])\n",
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "# def predict(dictionary,description,clf):\n",
    "#     a = dictionary.doc2bow(handle_desc(description))\n",
    "#     topic_distribute = lda[a]\n",
    "#     res = clf.predict(topic_distribute)\n",
    "#     return res\n",
    "\n",
    "\n",
    "# print(x_cut[0])\n",
    "# print(y[0])\n",
    "topic_num = 5\n",
    "a = load_dictionary.doc2bow(handle_desc(\"航空航天\"))\n",
    "topic_distribute = load_lda[a]\n",
    "print(topic_distribute)\n",
    "topics = np.zeros((1,topic_num))\n",
    "print(topics)\n",
    "for tupl in topic_distribute:\n",
    "    print(tupl)\n",
    "    topics[0,tupl[0]]=tupl[1]\n",
    "print(topics)\n",
    "res = load_clf.predict(topics)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补全地市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area():\n",
    "    df2 = pd.read_csv('qichacha.csv',index_col=0)\n",
    "    return df2['登记机关'].apply(lambda x:x[:2])\n",
    "    \n",
    "qichacha_cut['所在区域']=get_area()\n",
    "qichacha_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qichacha_cut.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_STR('\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import jieba\n",
    "\n",
    "with open('/root/DeepLearning/dict_save', 'rb') as f:\n",
    "    load_dictionary = pickle.load(f)   # read file and build object\n",
    "from gensim import models\n",
    "load_lda = models.ldamodel.LdaModel.load('lda.model')\n",
    "# 加载svm.pickle\n",
    "with open('/root/DeepLearning/svm.pickle', 'rb') as fr:\n",
    "    load_clf = pickle.load(fr)\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    with open('/root/DeepLearning/data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])\n",
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "# def predict(dictionary,description,clf):\n",
    "#     a = dictionary.doc2bow(handle_desc(description))\n",
    "#     topic_distribute = lda[a]\n",
    "#     res = clf.predict(topic_distribute)\n",
    "#     return res\n",
    "\n",
    "\n",
    "# print(x_cut[0])\n",
    "# print(y[0])\n",
    "topic_num = 5\n",
    "a = load_dictionary.doc2bow(handle_desc(\"航空航天\"))\n",
    "topic_distribute = load_lda[a]\n",
    "print(topic_distribute)\n",
    "topics = np.zeros((1,topic_num))\n",
    "print(topics)\n",
    "for tupl in topic_distribute:\n",
    "    print(tupl)\n",
    "    topics[0,tupl[0]]=tupl[1]\n",
    "print(topics)\n",
    "res = load_clf.predict(topics)\n",
    "res[0]\n",
    "','')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
