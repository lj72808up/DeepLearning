{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 使用Keras的callback避免过拟合\n",
    "\n",
    "#### 一. ModelChackpoint与Earlystoping  \n",
    "1. `EarlyStopping`回调, 在监控到验证集上的效果不再提升时, 停止神经网络训练. 需要配合`ModelCheckpoint`使用  \n",
    "2. `ModelCheckpoint`: 持续保存模型在每个epoch后的权重参数, 会覆盖上次epoch得到的权重, 而只保留最近的表现最好的模型的参数  \n",
    "3. 做法 : \n",
    "  1. 先声明callback的列表\n",
    "  2. 在`model.fit`上传进指定的callback\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/devkit/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',  # 使用val_acc指标决定是否early stopping\n",
    "        patience=1),    # 在验证集表现不在提升后, 在执行1轮\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = 'model_param.h5',\n",
    "        monitor = 'val_loss',    # 如下两个参数, 表示只要val_loss表现有提升, 就覆盖保存的参数文件\n",
    "        save_best_only = True    \n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (60000, 28, 28)\n",
      "labels:  [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# 1. 导入mnist数据集\n",
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "print 'train shape: ',train_images.shape  # ndarray\n",
    "# 2. 数据预处理\n",
    "train_images = train_images.reshape((60000,28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "# one-hot输出\n",
    "from keras import utils\n",
    "print 'labels: ',test_labels\n",
    "train_labels = utils.to_categorical(train_labels)\n",
    "test_labels = utils.to_categorical(test_labels)\n",
    "# 3. 构建网络\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "# input_shape : 输入张量的形状, (28*28,)表示1维度向量\n",
    "model.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.2223 - acc: 0.9343 - val_loss: 0.1235 - val_acc: 0.9638\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.0979 - acc: 0.9709 - val_loss: 0.1046 - val_acc: 0.9718\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.0686 - acc: 0.9808 - val_loss: 0.1017 - val_acc: 0.9722\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.0523 - acc: 0.9858 - val_loss: 0.1025 - val_acc: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb9664d890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 编译\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['acc'])\n",
    "\n",
    "# 5. 训练模型\n",
    "model.fit(train_images,train_labels,\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            callbacks = callbacks_list,\n",
    "            validation_split=0.2\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二. ReduceLROnPlateau callback\n",
    "1. `ReduceLROnPlateau`可以在验证集的'val_loss'表现不再提升时, 降低参数的学习率('learning rate'). 用来更精细的到达局部最优解\n",
    "2. ReduceLROnPlateau使用方法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',  # 监控指标\n",
    "        factor = 0.1,  # new_lr = lr * factor\n",
    "        patience = 10  # 验证机上的表现不再提升后, 再经过10轮epoch再降低学习率\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TensorBoard\n",
    "1. Tensorbord可视化训练过程的监控指标, 通过制定keras的callback对象为`keras.callbacks.TensorBoard`, 将训练过程中的监控指标输出到一个文件上. \n",
    "2. 然后使用**cli**读取文件\n",
    "```shell\n",
    "tensorboard --logdir=my_log_dir\n",
    "```\n",
    "3. 访问**url**  \n",
    "```http://localhost:6006```\n",
    "\n",
    "如下使用mnist举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape:', (60000, 28, 28, 1))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.2679 - acc: 0.9173 - val_loss: 0.0616 - val_acc: 0.9793\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0913 - acc: 0.9730 - val_loss: 0.0438 - val_acc: 0.9853\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0661 - acc: 0.9804 - val_loss: 0.0373 - val_acc: 0.9875\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0570 - acc: 0.9831 - val_loss: 0.0332 - val_acc: 0.9891\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0482 - acc: 0.9855 - val_loss: 0.0294 - val_acc: 0.9901\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 0.0302 - val_acc: 0.9903\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0375 - acc: 0.9883 - val_loss: 0.0296 - val_acc: 0.9898\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.0298 - val_acc: 0.9902\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0308 - acc: 0.9901 - val_loss: 0.0270 - val_acc: 0.9920\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0298 - acc: 0.9906 - val_loss: 0.0273 - val_acc: 0.9914\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0262 - acc: 0.9915 - val_loss: 0.0293 - val_acc: 0.9915\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0261 - acc: 0.9919 - val_loss: 0.0289 - val_acc: 0.9911\n",
      "('Test loss:', 0.028899025906383942)\n",
      "('Test accuracy:', 0.9911)\n"
     ]
    }
   ],
   "source": [
    "from os import makedirs\n",
    "from os.path import exists, join\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "log_dir = './logs'\n",
    "\n",
    "if not exists(log_dir):\n",
    "    makedirs(log_dir)\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# save class labels to disk to color data points in TensorBoard accordingly\n",
    "with open(join(log_dir, 'metadata.tsv'), 'w') as f:\n",
    "    np.savetxt(f, y_test)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "tensorboard = TensorBoard(batch_size=batch_size,\n",
    "                          embeddings_freq=1,\n",
    "                          embeddings_layer_names=['features'],\n",
    "                          embeddings_metadata='metadata.tsv',\n",
    "                          embeddings_data=x_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', name='features'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=[tensorboard],\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
