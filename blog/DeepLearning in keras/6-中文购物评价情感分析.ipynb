{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一. 购物评价情感分析\n",
    "1. 本数据集包含两万多条中文标注语料，涉及六个领域的评论数据  \n",
    " 对这些评论数据先分词, 再倒入与训练好的词向量, 构建RNN模型进行情感分析\n",
    "<img src='img/zhongwenpingluing.jpg' height='60%' width='60%'>\n",
    "\n",
    "#### 二. 数据预处理\n",
    "1. 将文本数据分词, 并构建标签   \n",
    " 形成ndaray: x=分词后的语句 , y=0/1\n",
    "2. 划分训练集和测试机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,记载数据文件\n",
    "import pandas as pd\n",
    "neg_df = pd.read_csv('data/neg.csv',header=None,index_col=None)\n",
    "pos_df = pd.read_csv('data/pos.csv',header=None,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...\n",
       "1  作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...\n",
       "2  作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...\n",
       "3       作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。\n",
       "4                  作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始的预料库负例\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...\n",
       "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...\n",
       "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...\n",
       "3  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...\n",
       "4  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始语料库正例\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.641 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>cut_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "      <td>[做, 父母, 一定, 刘墉, 心态, 不断, 学习, 不断, 进步, 不断, 补充, 新鲜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>[作者, 真有, 英国人, 严谨, 风格, 提出, 观点, 进行, 论述, 论证, 物理学,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>[作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 计算结果, 支持, 其新,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>[作者, 战, 之前, ＂, 拥抱, ＂, 令人, 叫绝, ．, 日本, 战败, 美军, 占...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>[作者, 少年, 时即, 喜, 阅读, 看出, 精读, 无数, 经典, 一个, 庞大, 内心...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
       "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...   \n",
       "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...   \n",
       "3  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...   \n",
       "4  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...   \n",
       "\n",
       "                                           cut_words  \n",
       "0  [做, 父母, 一定, 刘墉, 心态, 不断, 学习, 不断, 进步, 不断, 补充, 新鲜...  \n",
       "1  [作者, 真有, 英国人, 严谨, 风格, 提出, 观点, 进行, 论述, 论证, 物理学,...  \n",
       "2  [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 计算结果, 支持, 其新,...  \n",
       "3  [作者, 战, 之前, ＂, 拥抱, ＂, 令人, 叫绝, ．, 日本, 战败, 美军, 占...  \n",
       "4  [作者, 少年, 时即, 喜, 阅读, 看出, 精读, 无数, 经典, 一个, 庞大, 内心...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对dataframe增加分此后的列\n",
    "import jieba\n",
    "cw = lambda x: list(jieba.cut(x))  # jieba.cut返回generator\n",
    "stpword_path = 'data/stop_words.txt'\n",
    "# 生成utf-8的停用词表\n",
    "with open(stpword_path,'rb') as f:\n",
    "    content = f.read()\n",
    "    # 按行切分停用词表\n",
    "    stpword_list = content.splitlines()\n",
    "    stpword_list = [word.decode('GBK') for word in stpword_list]\n",
    "    \n",
    "def cw2(x):\n",
    "    generator = jieba.cut(x)\n",
    "    return [word for word in generator if word not in stpword_list]\n",
    "\n",
    "pos_df['cut_words'] = pos_df[0].apply(cw2)\n",
    "neg_df['cut_words'] = pos_df[0].apply(cw2)\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21105 sentences in corpus\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 构建语料库ndarray\n",
    "x = np.concatenate((pos_df['cut_words'],neg_df['cut_words']))\n",
    "# 构建标签ndarray:1为正标签,0为负标签\n",
    "y = np.concatenate((np.ones(len(pos_df)),np.zeros(len(neg_df))),axis = 0)\n",
    "\n",
    "sentences_num = x.shape[0]\n",
    "print(sentences_num,'sentences in corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "indices = np.arange(sentences_num)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三. 导入训练的词嵌入, 转换文本corpus为index corpus\n",
    "1. 预训练的词嵌入下载\n",
    "使用百度文库上, Skip-Gram with Negative Sampling (SGNS)训练的词向量. 文本1.7G,共635974词,300维  \n",
    "[https://github.com/Embedding/Chinese-Word-Vectors](https://github.com/Embedding/Chinese-Word-Vectors)\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 60001 words in embedding_dict\n"
     ]
    }
   ],
   "source": [
    "embedding_dir = '/home/lj/data/Deelearning_with_python/sgns.baidubaike.bigram-char'\n",
    "embedding_dict = {}\n",
    "from itertools import islice\n",
    "with open(embedding_dir,'r') as f:\n",
    "    for line_num,line in enumerate(f): #滤出文件第0行\n",
    "        if line_num>60000:\n",
    "            break\n",
    "        line = line.strip('\\n')\n",
    "        content = line.split(' ')\n",
    "        word = content[0]\n",
    "        coefs = np.array(content[1:301],dtype='float32')\n",
    "        word_num = line_num+1 # 词典编号从1开始,0号预留出来\n",
    "        embedding_dict[word] = (word_num,coefs)\n",
    "print('found %s words in embedding_dict'%len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, array([ 5.045200e-01, -1.705800e-01,  2.638530e-01,  5.306370e-01,\n",
       "         1.101530e-01, -2.658490e-01, -1.900760e-01,  7.454000e-03,\n",
       "         4.651710e-01,  7.582930e-01, -2.205990e-01, -4.840300e-02,\n",
       "        -2.912600e-01,  2.067500e-01, -5.822560e-01,  3.393050e-01,\n",
       "         1.283190e-01,  3.119600e-02, -3.328000e-03,  2.110470e-01,\n",
       "        -3.468420e-01,  4.714000e-03, -2.035070e-01,  6.305240e-01,\n",
       "         6.505300e-01, -1.291500e-02, -1.219030e-01,  1.085650e-01,\n",
       "        -7.396940e-01,  2.838400e-01,  5.618630e-01, -1.761840e-01,\n",
       "         7.858510e-01, -3.989040e-01,  1.630060e-01,  1.085489e+00,\n",
       "        -1.393830e-01,  2.006670e-01, -5.734050e-01,  9.604600e-01,\n",
       "         1.400230e-01, -1.248930e-01, -3.209100e-02, -4.257840e-01,\n",
       "         3.986000e-02, -4.035770e-01, -3.634310e-01,  3.756700e-01,\n",
       "        -9.476200e-02, -4.044900e-02,  8.362600e-02,  3.855650e-01,\n",
       "         1.052700e-01, -2.576370e-01, -3.927490e-01,  9.302000e-03,\n",
       "        -9.103500e-02, -1.662470e-01, -3.332070e-01,  6.599970e-01,\n",
       "         6.039100e-02,  2.053440e-01, -3.179970e-01,  2.617200e-01,\n",
       "         1.402970e-01,  1.019170e-01, -4.762740e-01, -2.496360e-01,\n",
       "         2.964360e-01, -5.069740e-01, -4.044590e-01,  1.677110e-01,\n",
       "         8.004160e-01,  2.810000e-03,  1.438820e-01,  2.453550e-01,\n",
       "        -1.562410e-01, -1.969750e-01,  1.370170e-01,  1.011020e-01,\n",
       "         2.063310e-01,  1.545070e-01,  5.409490e-01, -8.109340e-01,\n",
       "         3.307170e-01, -3.952230e-01, -1.619960e-01, -1.307420e-01,\n",
       "         5.258690e-01,  2.321730e-01,  7.966400e-02, -2.497850e-01,\n",
       "         8.082900e-02, -3.252200e-02, -2.246700e-01, -2.782980e-01,\n",
       "        -2.270850e-01, -1.834390e-01,  9.551100e-02,  1.775530e-01,\n",
       "        -2.811020e-01, -1.731000e-02, -1.502790e-01, -3.459250e-01,\n",
       "        -2.313230e-01,  2.125410e-01,  1.093560e-01, -6.978500e-02,\n",
       "        -6.818200e-02, -3.378550e-01, -8.320390e-01, -5.943540e-01,\n",
       "         9.917300e-02, -3.488120e-01,  6.831390e-01, -7.529000e-02,\n",
       "        -2.812330e-01, -3.473710e-01, -1.002237e+00,  2.324440e-01,\n",
       "         7.232900e-02,  2.056540e-01, -2.210650e-01, -4.494070e-01,\n",
       "         1.330780e-01,  4.747340e-01,  2.461180e-01, -6.732080e-01,\n",
       "        -3.880280e-01, -4.035370e-01, -3.435460e-01,  2.881710e-01,\n",
       "        -5.176400e-02,  1.307440e-01,  9.069200e-02,  3.075470e-01,\n",
       "        -1.014297e+00, -4.155320e-01, -3.880470e-01, -5.223920e-01,\n",
       "         4.755510e-01,  7.720040e-01, -7.800050e-01,  2.808880e-01,\n",
       "         1.643000e-01,  4.202560e-01,  1.006900e-02,  1.990210e-01,\n",
       "        -1.257130e-01,  1.553800e-01, -1.100430e-01,  5.508740e-01,\n",
       "        -5.617450e-01,  1.286200e-02,  1.850630e-01, -1.890660e-01,\n",
       "         1.272990e-01,  2.002640e-01,  3.070140e-01,  3.652230e-01,\n",
       "        -1.240350e-01, -8.844620e-01,  1.423640e-01, -3.256360e-01,\n",
       "        -4.697600e-02, -2.833250e-01, -3.966120e-01,  2.339300e-01,\n",
       "         9.164300e-02, -4.931530e-01, -3.111000e-02,  5.774030e-01,\n",
       "         1.740540e-01,  5.443240e-01,  3.324300e-02, -3.233950e-01,\n",
       "        -1.466660e-01,  9.405500e-02, -3.173860e-01, -2.399820e-01,\n",
       "         7.531700e-02, -1.686600e-01,  6.646140e-01,  9.128390e-01,\n",
       "         6.851000e-03, -1.065219e+00,  4.366570e-01, -2.427200e-01,\n",
       "         4.383740e-01, -4.921680e-01, -4.970550e-01,  6.744080e-01,\n",
       "        -1.020230e-01, -6.236180e-01,  2.736490e-01,  6.071220e-01,\n",
       "        -5.998040e-01,  5.124070e-01, -2.184540e-01, -1.063830e-01,\n",
       "         7.974600e-02, -6.918770e-01,  1.531280e-01, -1.023000e-03,\n",
       "         4.063550e-01,  1.971270e-01,  2.834110e-01,  2.189030e-01,\n",
       "        -1.322830e-01, -9.131300e-02, -4.165500e-02,  2.922970e-01,\n",
       "         7.549300e-01, -3.248920e-01, -6.158540e-01,  3.180720e-01,\n",
       "        -1.883860e-01,  1.570900e-02, -4.301000e-02, -2.457500e-02,\n",
       "         6.220510e-01,  6.078960e-01,  2.955030e-01, -3.525910e-01,\n",
       "         1.152740e-01,  1.003120e-01, -5.564610e-01, -1.414850e-01,\n",
       "        -9.168800e-02,  2.798500e-02, -4.300780e-01, -5.813200e-01,\n",
       "        -1.560890e-01,  3.426230e-01,  7.880000e-03, -2.198870e-01,\n",
       "         4.186260e-01,  2.922110e-01, -1.748950e-01,  6.110000e-04,\n",
       "         1.668360e-01, -4.352200e-02,  5.205820e-01, -2.203310e-01,\n",
       "         5.136880e-01, -6.726430e-01,  9.235200e-02, -4.808830e-01,\n",
       "        -5.617130e-01,  1.918810e-01,  4.143770e-01,  4.825460e-01,\n",
       "        -1.436800e-02, -3.979490e-01,  4.551000e-03, -8.216900e-02,\n",
       "        -5.329550e-01, -2.978630e-01, -3.162240e-01,  4.297620e-01,\n",
       "         7.963500e-02, -1.436440e-01,  6.434630e-01,  2.783080e-01,\n",
       "        -7.673000e-02,  2.603120e-01,  3.319810e-01, -2.839750e-01,\n",
       "         9.006300e-02,  4.645130e-01, -6.882500e-02, -5.125000e-02,\n",
       "        -1.111210e-01, -3.519840e-01, -9.854000e-03,  1.634170e-01,\n",
       "        -6.687980e-01, -1.298030e-01,  1.315140e-01, -1.813760e-01,\n",
       "         1.138100e-01, -3.000130e-01, -3.329600e-02, -1.987170e-01,\n",
       "        -2.038520e-01,  5.424070e-01,  8.235310e-01, -1.360020e-01,\n",
       "         6.631060e-01,  2.846600e-02, -3.259430e-01, -1.279570e-01,\n",
       "         2.917350e-01, -2.504420e-01,  2.780630e-01, -1.829880e-01,\n",
       "        -5.692970e-01, -2.249550e-01, -5.250900e-02,  3.172320e-01],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看形成的词嵌入词典中, '的'字符对应的值\n",
    "# (词典中的编号, 词向量)\n",
    "embedding_dict['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([807, 15983, 7397, 1013, 990, 7864, 4601, 2729, 5500, 281, 18479, 1380, 19969, 14549, 17894, 2957, 2238, 1729, 31149, 1679, 1363, 6724, 507, 458, 11621, 11479, 17991, 28, 4366, 19109, 784, 1722, 3316, 20969, 25188, 86, 19109, 1351, 686, 426, 2122, 3713, 35353, 210, 158, 830]),\n",
       "       list([5486, 4757, 619, 1645, 26856, 4208, 4208, 562, 4720, 4208, 1288, 462, 1631, 370, 5486, 370, 12231, 5486, 459, 49711, 13214, 47773, 1075, 807, 57981, 870, 15885, 53666])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将文本型的x, 转换成index型的x\n",
    "fun1 = lambda xi:[embedding_dict[w][0] for w in xi if w in embedding_dict]\n",
    "x = np.array([fun1(xi) for xi in x])\n",
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集,测试集\n",
    "train_samples = int(0.8*sentences_num)\n",
    "x_train = x[:train_samples]\n",
    "y_train = y[:train_samples]\n",
    "x_test = x[train_samples:]\n",
    "y_test = y[train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换embedding_dict为embedding_matrix\n",
    "n = 300  \n",
    "m = len(embedding_dict.keys()) + 1 # 0号index没有单词,只是占位符\n",
    "embedding_matrix = np.zeros((m,n))\n",
    "\n",
    "for item in embedding_dict.items():\n",
    "    val = item[1] # (index,embedding_vector)\n",
    "    embedding_matrix[val[0]] = val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三. 构建神经网络\n",
    "1. 把评论切成统一长度\n",
    "2. 后期优化  \n",
    " 1. 使用gensim自己训练词向量  \n",
    " 2. 使用fasttext词向量\n",
    " 3. 验证文字下x和y的匹配, 验证文字下x和index下x的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/devkit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Flatten,LSTM,Dense\n",
    "from keras import preprocessing\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "# sentence length\n",
    "maxlen = 100\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60002, 300)\n",
      "(16884, 100)\n",
      "(16884,)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   807 15983  7397  1013   990  7864\n",
      "  4601  2729  5500   281 18479  1380 19969 14549 17894  2957  2238  1729\n",
      " 31149  1679  1363  6724   507   458 11621 11479 17991    28  4366 19109\n",
      "   784  1722  3316 20969 25188    86 19109  1351   686   426  2122  3713\n",
      " 35353   210   158   830]\n",
      "60002\n",
      "300\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train[0])\n",
    "print(m)\n",
    "print(n)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          18000600  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 18,269,721\n",
      "Trainable params: 18,269,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(m,n,input_length=maxlen))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "# model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False  # 冻结Embedding层\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15195 samples, validate on 1689 samples\n",
      "Epoch 1/10\n",
      "15195/15195 [==============================] - 139s 9ms/step - loss: 0.6986 - acc: 0.5019 - val_loss: 0.6935 - val_acc: 0.4991\n",
      "Epoch 2/10\n",
      "15195/15195 [==============================] - 137s 9ms/step - loss: 0.6950 - acc: 0.5046 - val_loss: 0.6948 - val_acc: 0.4985\n",
      "Epoch 3/10\n",
      "15195/15195 [==============================] - 138s 9ms/step - loss: 0.6942 - acc: 0.5025 - val_loss: 0.6936 - val_acc: 0.4731\n",
      "Epoch 4/10\n",
      "15195/15195 [==============================] - 138s 9ms/step - loss: 0.6934 - acc: 0.5029 - val_loss: 0.6946 - val_acc: 0.4926\n",
      "Epoch 5/10\n",
      "15195/15195 [==============================] - 144s 9ms/step - loss: 0.6934 - acc: 0.5017 - val_loss: 0.6983 - val_acc: 0.4352\n",
      "Epoch 6/10\n",
      "15195/15195 [==============================] - 326s 21ms/step - loss: 0.6923 - acc: 0.5166 - val_loss: 0.7029 - val_acc: 0.4091\n",
      "Epoch 7/10\n",
      "15195/15195 [==============================] - 179s 12ms/step - loss: 0.6904 - acc: 0.5258 - val_loss: 0.7074 - val_acc: 0.4014\n",
      "Epoch 8/10\n",
      "15195/15195 [==============================] - 177s 12ms/step - loss: 0.6891 - acc: 0.5372 - val_loss: 0.7188 - val_acc: 0.3529\n",
      "Epoch 9/10\n",
      "15195/15195 [==============================] - 180s 12ms/step - loss: 0.6862 - acc: 0.5449 - val_loss: 0.7478 - val_acc: 0.3381\n",
      "Epoch 10/10\n",
      "15195/15195 [==============================] - 179s 12ms/step - loss: 0.6843 - acc: 0.5483 - val_loss: 0.7744 - val_acc: 0.3150\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221/4221 [==============================] - 28s 7ms/step\n",
      "Test score: [0.7713313046379469, 0.2961383558610298]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print ('Test score:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
