{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 企查查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.按照红头文件匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    with open('data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/lj/ideaprojects/DeepLearning/data/dict.txt.small ...\n",
      "Dumping model to file cache /tmp/jieba.ube6b3622f3ed3770dfb4b3dbec1af2a2.cache\n",
      "Loading model cost 0.270 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "\n",
    "    \n",
    "def hadle_official_info():\n",
    "    '''讲地市的红头文件分词'''\n",
    "    df = pd.read_csv('./redhead.csv',delimiter='$',header=-1)\n",
    "    df.head()\n",
    "    df[1] = df[1].apply(lambda x:strQ2B(x))\n",
    "    jieba.set_dictionary('data/dict.txt.small')\n",
    "    x_cut = [] #分词后,去除停用词的语料库\n",
    "\n",
    "    # 获取停用词列表\n",
    "    stopwords = get_stop_words()\n",
    "    cut_res = df[1].apply(cut)\n",
    "    df[1] = cut_res\n",
    "    return df\n",
    "\n",
    "official_info_df = hadle_official_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>济南</td>\n",
       "      <td>信息技术/智能/制造/高端/装备/量子/科技/生物/医药/先进/材料/产业/金融/现代/物流...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>青岛</td>\n",
       "      <td>西海岸/新区/蓝谷/核心区/高新区/胶东/临空/经济/示范区/胶州湾/青岛/老城区/有机/更...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>烟台</td>\n",
       "      <td>烟台/经济/技术开发区/烟台/高新技术/产业/开发区/烟台/东部/产城/融合/发展/示范区/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>淄博</td>\n",
       "      <td>国家/省级/经济/技术开发区/高新技术/产业/开发区/海关/特殊/监管/区/新/能源/电池/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>枣庄</td>\n",
       "      <td>人工智能/信息技术/新/能源/新/材料/医养/健康</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>东营</td>\n",
       "      <td>航空航天/石化/装备/新/能源/文化/旅游</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>潍坊</td>\n",
       "      <td>虚拟/现实/人工智能/新/能源/电池/高端/装备/制造/生物/基/材料/信息技术/现代/种业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>济宁</td>\n",
       "      <td>第三代/半导体/生命/健康/信息技术/文化/旅游/生物/医药</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>泰安</td>\n",
       "      <td>人工智能/生命/健康/信息技术/高端/装备/制造/文化/旅游/体育/新/能源</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>威海</td>\n",
       "      <td>生命/健康/前沿/新/材料/医疗/器械/海洋生物/时尚/创意</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>日照</td>\n",
       "      <td>生命/健康/通用/航空/文化/旅游/海洋生物/医药/现代/物流/高端/装备/制造</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>莱芜</td>\n",
       "      <td>航天/航空/服务/清洁/能源/冶金/新/材料/全域/旅游</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>临沂</td>\n",
       "      <td>生命/健康/航空航天/机器人/信息技术/磁性/材料/文化/旅游/新/能源/生物/医药/节能/环保</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>德州</td>\n",
       "      <td>生命/健康/航空航天/材料/新/能源/生物/医药/体育/高端/装备/制造</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>聊城</td>\n",
       "      <td>医养/健康/新/能源/汽车/新/材料/生物/医药</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>滨州</td>\n",
       "      <td>航空航天/材料/新/能源/电池/高端/装备/制造/高端/化工/新/能源</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>菏泽</td>\n",
       "      <td>生命/健康/高端/装备/新/材料/高端/化工/生物/医药/信息技术/节能/环保</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "0   济南  信息技术/智能/制造/高端/装备/量子/科技/生物/医药/先进/材料/产业/金融/现代/物流...\n",
       "1   青岛  西海岸/新区/蓝谷/核心区/高新区/胶东/临空/经济/示范区/胶州湾/青岛/老城区/有机/更...\n",
       "2   烟台  烟台/经济/技术开发区/烟台/高新技术/产业/开发区/烟台/东部/产城/融合/发展/示范区/...\n",
       "3   淄博  国家/省级/经济/技术开发区/高新技术/产业/开发区/海关/特殊/监管/区/新/能源/电池/...\n",
       "4   枣庄                          人工智能/信息技术/新/能源/新/材料/医养/健康\n",
       "5   东营                              航空航天/石化/装备/新/能源/文化/旅游\n",
       "6   潍坊     虚拟/现实/人工智能/新/能源/电池/高端/装备/制造/生物/基/材料/信息技术/现代/种业\n",
       "7   济宁                     第三代/半导体/生命/健康/信息技术/文化/旅游/生物/医药\n",
       "8   泰安             人工智能/生命/健康/信息技术/高端/装备/制造/文化/旅游/体育/新/能源\n",
       "9   威海                     生命/健康/前沿/新/材料/医疗/器械/海洋生物/时尚/创意\n",
       "10  日照           生命/健康/通用/航空/文化/旅游/海洋生物/医药/现代/物流/高端/装备/制造\n",
       "11  莱芜                       航天/航空/服务/清洁/能源/冶金/新/材料/全域/旅游\n",
       "12  临沂   生命/健康/航空航天/机器人/信息技术/磁性/材料/文化/旅游/新/能源/生物/医药/节能/环保\n",
       "13  德州               生命/健康/航空航天/材料/新/能源/生物/医药/体育/高端/装备/制造\n",
       "14  聊城                           医养/健康/新/能源/汽车/新/材料/生物/医药\n",
       "15  滨州                航空航天/材料/新/能源/电池/高端/装备/制造/高端/化工/新/能源\n",
       "16  菏泽            生命/健康/高端/装备/新/材料/高端/化工/生物/医药/信息技术/节能/环保"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['爱', '中国', '呵呵', '中国']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_desc(\"我爱你中国,哈哈,呵呵,中国\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_info_df.to_csv('redhead2.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_info_df = pd.read_csv('redhead2.csv',index_col=0)\n",
    "official_info_df.index\n",
    "official_info_df.loc['青岛','1']\n",
    "type(official_info_df.loc['青岛','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "青岛\n",
      "pattern  青岛   16 字符\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def judge_by_official(desc,location):\n",
    "    '''匹配红头文件, 存在则判定为新动能\n",
    "       -1 : 未找到红头文件\n",
    "       0: 匹配失败\n",
    "       1: 新动能匹配成功\n",
    "    '''\n",
    "    try:\n",
    "        print(location)\n",
    "        official_info_df.loc[location]\n",
    "    except KeyError: \n",
    "        return -1\n",
    "    official_info = official_info_df.loc[location,'1']\n",
    "    official_info = official_info.split('/')\n",
    "    crossinfo = [w for w in desc if w in official_info]\n",
    "    print('pattern ',location, ' ',len(crossinfo),'字符')\n",
    "    if len(crossinfo)>1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "desc = '青岛达能环保设备股份有限公司，成立于2006年10月，注册资金6300万元，占地面积15万平方米，生产建筑面积5.5万平方米，制造设备齐全，生产装备达到国内同行业领先水平，目前已具备年产值10亿元人民币的生产能力。青岛达能环保设备股份有限公司属锅炉辅机、环保设备制造行业。主要从事电站锅炉干式除渣系统产品、湿式除渣系统产品及烟气余热利用系统产品的研发、生产与销售。青岛达能环保设备股份有限公司为集研发、生产、销售为一体的国家高新技术企业、国家AAA级信用企业、山东省战略新兴产业重点企业、青岛企业技术中心、青岛民营科技创新先进单位。获得国家民营科技发展贡献奖、国家重点新产品、山东优秀节能成果奖等项目的荣誉称号和奖励。随着公司新业务的扩展，现面向社会诚招以下人才'\n",
    "desc = handle_desc(desc)\n",
    "location = '青岛'\n",
    "judge_by_official(desc,location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LDA匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "neg_cut = pd.read_csv('qichacha_cut.csv',index_col=0)\n",
    "neg_cut['经营范围'] = neg_cut['经营范围'].apply(lambda x:x.split(',')).values\n",
    "neg_df = pd.DataFrame({'公司名称':neg_cut['公司名'].apply(lambda x:x[:-2]),\n",
    "                    '经营范围':neg_cut['经营范围']})\n",
    "neg_df['企业分类'] = '旧动能'\n",
    "neg_df = neg_df.dropna(how=\"any\",axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司名称</th>\n",
       "      <th>经营范围</th>\n",
       "      <th>企业分类</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>刘向岗</td>\n",
       "      <td>[批发, 零售]</td>\n",
       "      <td>旧动能</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>安丘市兴建新型装饰材料制品厂</td>\n",
       "      <td>[加工, 销售]</td>\n",
       "      <td>旧动能</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>安丘市浩红小吃铺</td>\n",
       "      <td>[小吃, 服务, 依法, 须, 批准, 项目, 相关, 部门, 批准, 后方, 开展, 经营...</td>\n",
       "      <td>旧动能</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>滨州市滨城区建利纸制品批发部</td>\n",
       "      <td>[纸制品, 需, 许可, 经营, 许可证, 经营]</td>\n",
       "      <td>旧动能</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>滨州市滨城区倩倩化妆品店</td>\n",
       "      <td>[化妆品]</td>\n",
       "      <td>旧动能</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             公司名称                                               经营范围 企业分类\n",
       "0             刘向岗                                           [批发, 零售]  旧动能\n",
       "1  安丘市兴建新型装饰材料制品厂                                           [加工, 销售]  旧动能\n",
       "2        安丘市浩红小吃铺  [小吃, 服务, 依法, 须, 批准, 项目, 相关, 部门, 批准, 后方, 开展, 经营...  旧动能\n",
       "3  滨州市滨城区建利纸制品批发部                          [纸制品, 需, 许可, 经营, 许可证, 经营]  旧动能\n",
       "4    滨州市滨城区倩倩化妆品店                                              [化妆品]  旧动能"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_excel(\"pos.xlsx\")\n",
    "pos_df = pd.DataFrame({\"公司名称\":pos_df[\"公司名称\"],\n",
    "                       \"经营范围\":pos_df[\"经营范围\"],\n",
    "                       \"企业分类\":pos_df[\"企业分类\"]})\n",
    "pos_df = pos_df.dropna(how=\"any\",axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司名称</th>\n",
       "      <th>经营范围</th>\n",
       "      <th>企业分类</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>济南二机床集团有限公司技术中心</td>\n",
       "      <td>[道路, 货物, 运输, 普通, 货运, 三类, 汽车, 维修, 车身, 清洁, 维护, 第...</td>\n",
       "      <td>高端装备制造</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>浪潮集团有限公司技术中心</td>\n",
       "      <td>[商用, 密码, 产品, 开发, 生产, 销售, 有效, 期限, 许可证, 为准, 计算机,...</td>\n",
       "      <td>信息产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鲁南制药集团股份有限公司技术中心</td>\n",
       "      <td>[生产, 销售, 原料药, 包装, 物品, 依法, 须经, 批准, 项目, 相关, 部门, ...</td>\n",
       "      <td>医养健康</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>济南轻骑摩托车有限公司技术中心</td>\n",
       "      <td>[摩托车, 零配件, 设计, 开发, 生产, 销售, 摩托车, 技术, 咨询, 服务, 引进...</td>\n",
       "      <td>高端装备制造</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>万华化学集团股份有限公司技术中心</td>\n",
       "      <td>[安全, 生产, 许可证, 范围, 化学, 危险品, 生产, 许可证, 范围, 铁路, 专用...</td>\n",
       "      <td>高端化工</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               公司名称                                               经营范围    企业分类\n",
       "0   济南二机床集团有限公司技术中心  [道路, 货物, 运输, 普通, 货运, 三类, 汽车, 维修, 车身, 清洁, 维护, 第...  高端装备制造\n",
       "1      浪潮集团有限公司技术中心  [商用, 密码, 产品, 开发, 生产, 销售, 有效, 期限, 许可证, 为准, 计算机,...    信息产业\n",
       "2  鲁南制药集团股份有限公司技术中心  [生产, 销售, 原料药, 包装, 物品, 依法, 须经, 批准, 项目, 相关, 部门, ...    医养健康\n",
       "3   济南轻骑摩托车有限公司技术中心  [摩托车, 零配件, 设计, 开发, 生产, 销售, 摩托车, 技术, 咨询, 服务, 引进...  高端装备制造\n",
       "4  万华化学集团股份有限公司技术中心  [安全, 生产, 许可证, 范围, 化学, 危险品, 生产, 许可证, 范围, 铁路, 专用...    高端化工"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_desc = pos_df[\"经营范围\"].dropna()\n",
    "pos_df[\"经营范围\"] = pos_df[\"经营范围\"].apply(cut)\n",
    "pos_df[\"经营范围\"] = pos_df[\"经营范围\"].apply(lambda x:x.split(\"/\"))\n",
    "# y = np.concatenate((np.ones(pos_desc.shape[0]),np.zeros(x_cut.shape[0])),axis=0)\n",
    "# x_cut = np.concatenate((pos_desc,x_cut),axis=0)\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8313, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非SVM数据处理\n",
    "new_pos = pos_df.copy()\n",
    "for i in range(50):\n",
    "    pos_df = pd.concat((new_pos,pos_df))\n",
    "pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9463, 3)\n",
      "['信息产业' '医养健康' '文化产业' '旅游产业' '旧动能' '海洋经济' '现代农业' '现代金融' '能源原材料' '能源新材料'\n",
      " '高端化工' '高端装备制造']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公司名称</th>\n",
       "      <th>经营范围</th>\n",
       "      <th>企业分类</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>济南二机床集团有限公司技术中心</td>\n",
       "      <td>[道路, 货物, 运输, 普通, 货运, 三类, 汽车, 维修, 车身, 清洁, 维护, 第...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>浪潮集团有限公司技术中心</td>\n",
       "      <td>[商用, 密码, 产品, 开发, 生产, 销售, 有效, 期限, 许可证, 为准, 计算机,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鲁南制药集团股份有限公司技术中心</td>\n",
       "      <td>[生产, 销售, 原料药, 包装, 物品, 依法, 须经, 批准, 项目, 相关, 部门, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>济南轻骑摩托车有限公司技术中心</td>\n",
       "      <td>[摩托车, 零配件, 设计, 开发, 生产, 销售, 摩托车, 技术, 咨询, 服务, 引进...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>万华化学集团股份有限公司技术中心</td>\n",
       "      <td>[安全, 生产, 许可证, 范围, 化学, 危险品, 生产, 许可证, 范围, 铁路, 专用...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               公司名称                                               经营范围  企业分类\n",
       "0   济南二机床集团有限公司技术中心  [道路, 货物, 运输, 普通, 货运, 三类, 汽车, 维修, 车身, 清洁, 维护, 第...    11\n",
       "1      浪潮集团有限公司技术中心  [商用, 密码, 产品, 开发, 生产, 销售, 有效, 期限, 许可证, 为准, 计算机,...     0\n",
       "2  鲁南制药集团股份有限公司技术中心  [生产, 销售, 原料药, 包装, 物品, 依法, 须经, 批准, 项目, 相关, 部门, ...     1\n",
       "3   济南轻骑摩托车有限公司技术中心  [摩托车, 零配件, 设计, 开发, 生产, 销售, 摩托车, 技术, 咨询, 服务, 引进...    11\n",
       "4  万华化学集团股份有限公司技术中心  [安全, 生产, 许可证, 范围, 化学, 危险品, 生产, 许可证, 范围, 铁路, 专用...    10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全部训练集\n",
    "df = pd.concat((pos_df,neg_df))\n",
    "print(df.shape)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[\"企业分类\"] = le.fit_transform(df[\"企业分类\"])\n",
    "print(le.classes_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3025 unique tokens: ['GC2', '三类', '专用', '业务', '为准']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(21,\n",
       "  '0.026*\"销售\" + 0.024*\"批准\" + 0.021*\"生产\" + 0.020*\"技术\" + 0.015*\"设备\" + 0.015*\"相关\" + 0.014*\"项目\" + 0.014*\"活动\" + 0.013*\"经营\" + 0.012*\"依法\"'),\n",
       " (16,\n",
       "  '0.030*\"批准\" + 0.026*\"销售\" + 0.021*\"经营\" + 0.019*\"服务\" + 0.017*\"开展\" + 0.016*\"相关\" + 0.014*\"港口\" + 0.014*\"活动\" + 0.014*\"种植\" + 0.014*\"许可证\"'),\n",
       " (13,\n",
       "  '0.035*\"服务\" + 0.030*\"销售\" + 0.030*\"批准\" + 0.027*\"生产\" + 0.021*\"设备\" + 0.020*\"相关\" + 0.018*\"许可证\" + 0.018*\"技术\" + 0.015*\"经营\" + 0.015*\"项目\"'),\n",
       " (4,\n",
       "  '0.037*\"批准\" + 0.034*\"销售\" + 0.029*\"经营\" + 0.022*\"相关\" + 0.021*\"服务\" + 0.020*\"开展\" + 0.020*\"生产\" + 0.019*\"活动\" + 0.018*\"部门\" + 0.018*\"许可证\"'),\n",
       " (8,\n",
       "  '0.027*\"经营\" + 0.026*\"批准\" + 0.022*\"设备\" + 0.016*\"技术\" + 0.014*\"工程\" + 0.013*\"开展\" + 0.013*\"部门\" + 0.013*\"销售\" + 0.012*\"后方\" + 0.012*\"机械\"'),\n",
       " (17,\n",
       "  '0.041*\"销售\" + 0.028*\"批准\" + 0.021*\"设备\" + 0.020*\"相关\" + 0.018*\"服务\" + 0.017*\"经营\" + 0.017*\"项目\" + 0.017*\"工程\" + 0.016*\"进出口\" + 0.015*\"活动\"'),\n",
       " (22,\n",
       "  '0.042*\"批准\" + 0.037*\"设备\" + 0.028*\"机械\" + 0.024*\"相关\" + 0.023*\"开展\" + 0.023*\"项目\" + 0.023*\"经营\" + 0.023*\"活动\" + 0.022*\"部门\" + 0.021*\"依法\"'),\n",
       " (19,\n",
       "  '0.046*\"批准\" + 0.032*\"相关\" + 0.028*\"经营\" + 0.025*\"项目\" + 0.024*\"生产\" + 0.023*\"部门\" + 0.023*\"开展\" + 0.023*\"依法\" + 0.023*\"肥料\" + 0.022*\"后方\"'),\n",
       " (12,\n",
       "  '0.033*\"批准\" + 0.027*\"经营\" + 0.026*\"生产\" + 0.026*\"设备\" + 0.025*\"销售\" + 0.020*\"项目\" + 0.018*\"技术\" + 0.018*\"依法\" + 0.017*\"活动\" + 0.017*\"开展\"'),\n",
       " (2,\n",
       "  '0.037*\"销售\" + 0.037*\"批准\" + 0.030*\"生产\" + 0.026*\"经营\" + 0.022*\"项目\" + 0.018*\"相关\" + 0.017*\"许可证\" + 0.017*\"部门\" + 0.016*\"设备\" + 0.015*\"依法\"'),\n",
       " (11,\n",
       "  '0.030*\"批准\" + 0.023*\"配件\" + 0.022*\"经营\" + 0.022*\"项目\" + 0.021*\"相关\" + 0.019*\"器材\" + 0.018*\"后方\" + 0.018*\"销售\" + 0.018*\"活动\" + 0.017*\"设施\"'),\n",
       " (9,\n",
       "  '0.034*\"批准\" + 0.031*\"经营\" + 0.023*\"项目\" + 0.022*\"销售\" + 0.022*\"生产\" + 0.019*\"开展\" + 0.018*\"活动\" + 0.017*\"服务\" + 0.016*\"依法\" + 0.015*\"部门\"'),\n",
       " (5,\n",
       "  '0.043*\"经营\" + 0.038*\"销售\" + 0.031*\"批准\" + 0.022*\"设备\" + 0.021*\"生产\" + 0.019*\"技术\" + 0.018*\"项目\" + 0.018*\"活动\" + 0.017*\"业务\" + 0.017*\"服务\"'),\n",
       " (1,\n",
       "  '0.033*\"批准\" + 0.031*\"经营\" + 0.028*\"技术\" + 0.025*\"销售\" + 0.023*\"设备\" + 0.021*\"项目\" + 0.020*\"业务\" + 0.020*\"相关\" + 0.019*\"生产\" + 0.018*\"服务\"'),\n",
       " (6,\n",
       "  '0.037*\"经营\" + 0.035*\"生产\" + 0.027*\"批准\" + 0.026*\"许可证\" + 0.024*\"肥料\" + 0.020*\"项目\" + 0.018*\"开展\" + 0.018*\"销售\" + 0.018*\"食品\" + 0.016*\"活动\"'),\n",
       " (7,\n",
       "  '0.037*\"批准\" + 0.029*\"经营\" + 0.024*\"销售\" + 0.023*\"零售\" + 0.022*\"相关\" + 0.020*\"项目\" + 0.020*\"部门\" + 0.019*\"后方\" + 0.019*\"开展\" + 0.018*\"批发\"'),\n",
       " (3,\n",
       "  '0.048*\"产品\" + 0.045*\"相关\" + 0.032*\"批准\" + 0.026*\"经营\" + 0.022*\"项目\" + 0.021*\"技术\" + 0.018*\"开展\" + 0.018*\"销售\" + 0.017*\"开发\" + 0.016*\"活动\"'),\n",
       " (0,\n",
       "  '0.038*\"销售\" + 0.034*\"生产\" + 0.033*\"批准\" + 0.026*\"经营\" + 0.021*\"肥料\" + 0.019*\"相关\" + 0.018*\"活动\" + 0.018*\"部门\" + 0.017*\"项目\" + 0.017*\"后方\"'),\n",
       " (24,\n",
       "  '0.040*\"经营\" + 0.039*\"批准\" + 0.033*\"生产\" + 0.027*\"项目\" + 0.025*\"销售\" + 0.024*\"食品\" + 0.022*\"活动\" + 0.021*\"相关\" + 0.020*\"部门\" + 0.020*\"开展\"'),\n",
       " (14,\n",
       "  '0.042*\"销售\" + 0.040*\"批准\" + 0.037*\"经营\" + 0.026*\"生产\" + 0.023*\"相关\" + 0.020*\"设备\" + 0.019*\"依法\" + 0.018*\"项目\" + 0.018*\"须经\" + 0.018*\"开展\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLDA(topic_num):\n",
    "    from gensim.models import LdaMulticore\n",
    "    from gensim.corpora import Dictionary\n",
    "    # gensim\n",
    "    dictionary = Dictionary(df[\"经营范围\"])\n",
    "    print(dictionary)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df[\"经营范围\"]]\n",
    "    lda = LdaMulticore(corpus=corpus,  # LDA训练语料\n",
    "                   id2word=dictionary, # id到单词的映射表\n",
    "                   num_topics=topic_num)      # LDA主题数量 \n",
    "    return lda,corpus,dictionary\n",
    "topic_num = 25\n",
    "lda,corpus,dictionary = getLDA(topic_num)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 0.99428564)]\n",
      "9463\n"
     ]
    }
   ],
   "source": [
    "def get_topic_distribute(lda,doc):\n",
    "    return lda[doc]\n",
    "\n",
    "def get_all_doc_topic():\n",
    "    lda_matrix = np.zeros((df[\"经营范围\"].shape[0],topic_num),dtype='float64')\n",
    "    for index,doc in enumerate(corpus):\n",
    "        topics = get_topic_distribute(lda,corpus[index])\n",
    "        for topic_tuple in topics:\n",
    "            lda_matrix[index,topic_tuple[0]] = topic_tuple[1]\n",
    "    return lda_matrix\n",
    "\n",
    "print(get_topic_distribute(lda,corpus[0]))\n",
    "print(len(corpus))\n",
    "# 所有样本主题分布\n",
    "all_topic = get_all_doc_topic()\n",
    "# x_neg = all_topic[:1150]\n",
    "# x_pos = all_topic[1150:]\n",
    "# # svm正例\n",
    "# y_neg = np.zeros(x_neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9463, 25) (9463, 12)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "x = all_topic\n",
    "y = pd.get_dummies(df[\"企业分类\"])\n",
    "print(x.shape,y.shape)\n",
    "print(len(le.classes_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "9463/9463 [==============================] - 3s 313us/step - loss: 1.8044 - acc: 0.3931\n",
      "Epoch 2/60\n",
      "9463/9463 [==============================] - 3s 295us/step - loss: 1.5229 - acc: 0.4531\n",
      "Epoch 3/60\n",
      "9463/9463 [==============================] - 3s 295us/step - loss: 1.4202 - acc: 0.4881\n",
      "Epoch 4/60\n",
      "9463/9463 [==============================] - 3s 295us/step - loss: 1.3367 - acc: 0.5231\n",
      "Epoch 5/60\n",
      "9463/9463 [==============================] - 3s 296us/step - loss: 1.2751 - acc: 0.5396\n",
      "Epoch 6/60\n",
      "9463/9463 [==============================] - 3s 294us/step - loss: 1.2310 - acc: 0.5506\n",
      "Epoch 7/60\n",
      "9463/9463 [==============================] - 3s 299us/step - loss: 1.2072 - acc: 0.5590\n",
      "Epoch 8/60\n",
      "9463/9463 [==============================] - 3s 298us/step - loss: 1.1719 - acc: 0.5773\n",
      "Epoch 9/60\n",
      "9463/9463 [==============================] - 3s 301us/step - loss: 1.1491 - acc: 0.5836\n",
      "Epoch 10/60\n",
      "9463/9463 [==============================] - 3s 301us/step - loss: 1.1240 - acc: 0.5886\n",
      "Epoch 11/60\n",
      "9463/9463 [==============================] - 3s 300us/step - loss: 1.1038 - acc: 0.5962\n",
      "Epoch 12/60\n",
      "9463/9463 [==============================] - 3s 300us/step - loss: 1.0959 - acc: 0.5993\n",
      "Epoch 13/60\n",
      "9463/9463 [==============================] - 3s 300us/step - loss: 1.0812 - acc: 0.6095\n",
      "Epoch 14/60\n",
      "9463/9463 [==============================] - 3s 302us/step - loss: 1.0679 - acc: 0.6162\n",
      "Epoch 15/60\n",
      "9463/9463 [==============================] - 3s 304us/step - loss: 1.0576 - acc: 0.6198\n",
      "Epoch 16/60\n",
      "9463/9463 [==============================] - 3s 308us/step - loss: 1.0472 - acc: 0.6265\n",
      "Epoch 17/60\n",
      "9463/9463 [==============================] - 3s 305us/step - loss: 1.0385 - acc: 0.6282\n",
      "Epoch 18/60\n",
      "9463/9463 [==============================] - 3s 308us/step - loss: 1.0310 - acc: 0.6345\n",
      "Epoch 19/60\n",
      "9463/9463 [==============================] - 3s 310us/step - loss: 1.0261 - acc: 0.6301\n",
      "Epoch 20/60\n",
      "9463/9463 [==============================] - 3s 307us/step - loss: 1.0214 - acc: 0.6425\n",
      "Epoch 21/60\n",
      "9463/9463 [==============================] - 3s 311us/step - loss: 1.0143 - acc: 0.6464\n",
      "Epoch 22/60\n",
      "9463/9463 [==============================] - 3s 308us/step - loss: 1.0039 - acc: 0.6449\n",
      "Epoch 23/60\n",
      "9463/9463 [==============================] - 3s 307us/step - loss: 1.0049 - acc: 0.6494\n",
      "Epoch 24/60\n",
      "9463/9463 [==============================] - 3s 310us/step - loss: 0.9912 - acc: 0.6511\n",
      "Epoch 25/60\n",
      "9463/9463 [==============================] - 3s 336us/step - loss: 0.9980 - acc: 0.6476\n",
      "Epoch 26/60\n",
      "9463/9463 [==============================] - 3s 311us/step - loss: 0.9912 - acc: 0.6533\n",
      "Epoch 27/60\n",
      "9463/9463 [==============================] - 3s 310us/step - loss: 0.9988 - acc: 0.6555\n",
      "Epoch 28/60\n",
      "9463/9463 [==============================] - 3s 316us/step - loss: 0.9882 - acc: 0.6533 0s - loss: 0.9873\n",
      "Epoch 29/60\n",
      "9463/9463 [==============================] - 3s 313us/step - loss: 0.9946 - acc: 0.6588\n",
      "Epoch 30/60\n",
      "9463/9463 [==============================] - 3s 320us/step - loss: 0.9885 - acc: 0.6585\n",
      "Epoch 31/60\n",
      "9463/9463 [==============================] - 3s 320us/step - loss: 0.9815 - acc: 0.6622\n",
      "Epoch 32/60\n",
      "9463/9463 [==============================] - 3s 317us/step - loss: 0.9733 - acc: 0.6654\n",
      "Epoch 33/60\n",
      "9463/9463 [==============================] - 3s 318us/step - loss: 0.9726 - acc: 0.6649\n",
      "Epoch 34/60\n",
      "9463/9463 [==============================] - 3s 322us/step - loss: 0.9709 - acc: 0.6651\n",
      "Epoch 35/60\n",
      "9463/9463 [==============================] - 3s 325us/step - loss: 0.9602 - acc: 0.6654\n",
      "Epoch 36/60\n",
      "9463/9463 [==============================] - 3s 357us/step - loss: 0.9638 - acc: 0.6672\n",
      "Epoch 37/60\n",
      "9463/9463 [==============================] - 3s 325us/step - loss: 0.9652 - acc: 0.6722\n",
      "Epoch 38/60\n",
      "9463/9463 [==============================] - 3s 323us/step - loss: 0.9591 - acc: 0.6703\n",
      "Epoch 39/60\n",
      "9463/9463 [==============================] - 3s 336us/step - loss: 0.9529 - acc: 0.6743\n",
      "Epoch 40/60\n",
      "9463/9463 [==============================] - 3s 347us/step - loss: 0.9533 - acc: 0.6741\n",
      "Epoch 41/60\n",
      "9463/9463 [==============================] - 3s 323us/step - loss: 0.9581 - acc: 0.6799\n",
      "Epoch 42/60\n",
      "9463/9463 [==============================] - 3s 335us/step - loss: 0.9638 - acc: 0.6754\n",
      "Epoch 43/60\n",
      "9463/9463 [==============================] - 3s 343us/step - loss: 0.9650 - acc: 0.6760\n",
      "Epoch 44/60\n",
      "9463/9463 [==============================] - 3s 343us/step - loss: 0.9610 - acc: 0.6761\n",
      "Epoch 45/60\n",
      "9463/9463 [==============================] - 3s 327us/step - loss: 0.9684 - acc: 0.6802\n",
      "Epoch 46/60\n",
      "9463/9463 [==============================] - 3s 327us/step - loss: 0.9652 - acc: 0.6765\n",
      "Epoch 47/60\n",
      "9463/9463 [==============================] - 3s 328us/step - loss: 0.9743 - acc: 0.6801\n",
      "Epoch 48/60\n",
      "9463/9463 [==============================] - 3s 324us/step - loss: 0.9682 - acc: 0.6770\n",
      "Epoch 49/60\n",
      "9463/9463 [==============================] - 3s 330us/step - loss: 0.9762 - acc: 0.6791\n",
      "Epoch 50/60\n",
      "9463/9463 [==============================] - 3s 327us/step - loss: 0.9720 - acc: 0.6779\n",
      "Epoch 51/60\n",
      "9463/9463 [==============================] - 3s 327us/step - loss: 0.9711 - acc: 0.6783\n",
      "Epoch 52/60\n",
      "9463/9463 [==============================] - 3s 345us/step - loss: 0.9826 - acc: 0.6795\n",
      "Epoch 53/60\n",
      "9463/9463 [==============================] - 4s 373us/step - loss: 0.9808 - acc: 0.6808\n",
      "Epoch 54/60\n",
      "9463/9463 [==============================] - 4s 418us/step - loss: 0.9731 - acc: 0.6804\n",
      "Epoch 55/60\n",
      "9463/9463 [==============================] - 3s 331us/step - loss: 0.9773 - acc: 0.6798\n",
      "Epoch 56/60\n",
      "9463/9463 [==============================] - 3s 333us/step - loss: 0.9720 - acc: 0.6803\n",
      "Epoch 57/60\n",
      "9463/9463 [==============================] - 3s 342us/step - loss: 0.9620 - acc: 0.6877\n",
      "Epoch 58/60\n",
      "9463/9463 [==============================] - 3s 333us/step - loss: 0.9710 - acc: 0.6840\n",
      "Epoch 59/60\n",
      "9463/9463 [==============================] - 3s 330us/step - loss: 0.9644 - acc: 0.6898\n",
      "Epoch 60/60\n",
      "9463/9463 [==============================] - 3s 331us/step - loss: 0.9510 - acc: 0.6873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f111c2787f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "# input_shape : 输入张量的形状, (28*28,)表示1维度向量\n",
    "network.add(layers.Dense(32,activation='relu',input_shape=(x.shape[1],)))\n",
    "network.add(layers.Dense(len(le.classes_),activation='softmax'))\n",
    "# 4. 编译\n",
    "network.compile(optimizer = 'rmsprop',\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# 5. 训练模型\n",
    "network.fit(x,y,epochs=60,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM多分类\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_svm(lda_matrix,label):\n",
    "    clf = SVC(probability=True)\n",
    "    clf.fit(lda_matrix, label) \n",
    "    return clf\n",
    "\n",
    "\n",
    "clfs = {}\n",
    "for col in label_df.columns:\n",
    "    print(col)\n",
    "    df = pd.concat((topic_df,label_df[col]),axis=1)\n",
    "    new_df = df[df[col]==1]\n",
    "#     print(new_df.shape)\n",
    "#     print(df.shape)\n",
    "    # 正例膨胀100倍\n",
    "    for j in range(100):\n",
    "        df = pd.concat((df,new_df))\n",
    "    x = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    clf = train_svm(x_train,y_train)\n",
    "    clfs[col] = clf\n",
    "    print(clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#保存词典\n",
    "with open('dict_save', 'wb') as f:                     \n",
    "    picklestring = pickle.dump(dictionary, f)\n",
    "#保存lda模型\n",
    "lda.save('lda.model')\n",
    "#保存svm模型\n",
    "with open('clfs.pickle', 'wb') as fw:\n",
    "    pickle.dump(clfs, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import jieba\n",
    "\n",
    "with open('dict_save', 'rb') as f:\n",
    "    load_dictionary = pickle.load(f)   # read file and build object\n",
    "from gensim import models\n",
    "load_lda = models.ldamodel.LdaModel.load('lda.model')\n",
    "# 加载svm.pickle\n",
    "with open('clfs.pickle', 'rb') as fr:\n",
    "    clfs = pickle.load(fr)\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    with open('data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])\n",
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "# def predict(dictionary,description,clf):\n",
    "#     a = dictionary.doc2bow(handle_desc(description))\n",
    "#     topic_distribute = lda[a]\n",
    "#     res = clf.predict(topic_distribute)\n",
    "#     return res\n",
    "\n",
    "# desc = \"重金属污染\"\n",
    "desc = \" 计算机及外部设备、电力自动化及工业自动化控制系统、电子产品及通讯设备、电子元器件、计算机软硬件、机房设施、仪器仪表、汽车电器的开发、生产、销售及技术咨询服务;机械工程、塑料注塑模具和注塑件、建筑智能化及建筑节能工程设计与施工;新能源汽车充换电设备的研发、生产、销售和服务;地面卫星接收站、绕线机、铣曲机、衡器的制造、销售;节能技术的研发、运维、服务、转让;光伏发电的设计、研发、建设、维护及技术咨询;售电业务;进出口业务及对外经济技术合作业务。(依法须经批准的项目,经相关部门批准后方可开展经营活动)\"\n",
    "# print(y[0])\n",
    "topic_num = 25\n",
    "a = load_dictionary.doc2bow(handle_desc(desc))\n",
    "topic_distribute = load_lda[a]\n",
    "print(topic_distribute)\n",
    "topics = np.zeros((1,topic_num))\n",
    "print(topics)\n",
    "for tupl in topic_distribute:\n",
    "    print(tupl)\n",
    "    topics[0,tupl[0]]=tupl[1]\n",
    "print(topics[0,:])\n",
    "res = {}\n",
    "for clf_name,clf_model in clfs.items():\n",
    "    clf_res = clf_model.predict_proba(topics)\n",
    "    if clf_name != '旧动能':\n",
    "        res[clf_res[0,:][1]] = clf_name\n",
    "    print(clf_name+\" :\",clf_res[0,:])\n",
    "max_score = sorted(res.keys(),reverse=True)[0]\n",
    "if max_score<0.5:\n",
    "    label = \"旧动能\"\n",
    "else:\n",
    "    label = (res[max_score])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补全地市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area():\n",
    "    df2 = pd.read_csv('qichacha.csv',index_col=0)\n",
    "    return df2['登记机关'].apply(lambda x:x[:2])\n",
    "    \n",
    "qichacha_cut['所在区域']=get_area()\n",
    "qichacha_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qichacha_cut.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_STR('\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import jieba\n",
    "\n",
    "with open('/root/DeepLearning/dict_save', 'rb') as f:\n",
    "    load_dictionary = pickle.load(f)   # read file and build object\n",
    "from gensim import models\n",
    "load_lda = models.ldamodel.LdaModel.load('lda.model')\n",
    "# 加载svm.pickle\n",
    "with open('/root/DeepLearning/svm.pickle', 'rb') as fr:\n",
    "    load_clf = pickle.load(fr)\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    with open('/root/DeepLearning/data/stop_words_utf8.txt') as f:\n",
    "        content = f.readlines()\n",
    "        stopwords = [w.strip() for w in content]\n",
    "        return stopwords\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    '''全角转半角\n",
    "    ustring : 需要转换的字符串\n",
    "    '''\n",
    "    ss = ''\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # 全角空格直接转换\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss = ss + rstring\n",
    "    return ss\n",
    "# 对corpus中每个文章分词后滤出停用词\n",
    "def cut(doc):\n",
    "    content = list(jieba.cut(doc))\n",
    "    stopwords = get_stop_words()\n",
    "    return '/'.join([w for w in content if w not in stopwords and w!=' '])\n",
    "def handle_desc(desc):\n",
    "    desc = strQ2B(desc)\n",
    "    cut = jieba.cut(desc)\n",
    "    stopwords = get_stop_words()\n",
    "    return [w for w in cut if w not in stopwords and w!=' ']\n",
    "# def predict(dictionary,description,clf):\n",
    "#     a = dictionary.doc2bow(handle_desc(description))\n",
    "#     topic_distribute = lda[a]\n",
    "#     res = clf.predict(topic_distribute)\n",
    "#     return res\n",
    "\n",
    "\n",
    "# print(x_cut[0])\n",
    "# print(y[0])\n",
    "topic_num = 25\n",
    "a = load_dictionary.doc2bow(handle_desc(\"航空航天\"))\n",
    "topic_distribute = load_lda[a]\n",
    "print(topic_distribute)\n",
    "topics = np.zeros((1,topic_num))\n",
    "print(topics)\n",
    "for tupl in topic_distribute:\n",
    "    print(tupl)\n",
    "    topics[0,tupl[0]]=tupl[1]\n",
    "print(topics)\n",
    "res = load_clf.predict(topics)\n",
    "res[0]\n",
    "','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_excel(\"pos.xlsx\").to_csv(\"pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
