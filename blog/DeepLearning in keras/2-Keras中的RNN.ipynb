{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 keras中的RNN\n",
    "#### 一. SimpleRNN in keras\n",
    "1. SimpleRNN的输入  \n",
    " SimpleRNN同时处理多条时序数据, 所以输入SimpleRNN的数据是3D张量 ( `batch_size`, `timestamps`, `input_features` )  \n",
    " 而不是2D张量 ( `timestamps`, `input_features` )\n",
    "2. SimpleRNN的输出  \n",
    " keras中所有的RNN层都有两种模式  \n",
    "  1. 时序数据中, 每个时间步都有输出. 进而形成3D张量 ( `batch_size`,`timestamps`,`output_features` )  \n",
    "  2. 每个输入序列, 只在最后产生一个输出. 形成2D张量 ( `batch_size`,`output_features` )\n",
    "3. 现在, 我们使用SimpleRNN来训练`imdb`数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SimpleRNN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二. LSTM in keras\n",
    "1. 用LSTM解决imdb问题  \n",
    "2. LSTM在imdb上的表现解释 :  \n",
    "  1. 如上, 我们使用LSTM处理imdb数据集, 准确率达到89%. 比simplernn好很多, 这是因为LSTM可以处理长倚赖的梯度消失问题.    \n",
    "  2. 同时, 也比直接用onehot编码后(向量中元素值为0/1)使用全连接网络训练的结果好.虽然全连接时考虑了语句中的所有单词, 而这里我们只是将每个句子拆成500个时间步(timestep).  \n",
    "  3. 但是这个表现并不具有开创性, 原因可能是:   \n",
    "    1. 没有调节用于LSTM的word embedding  \n",
    "    2. 缺少正则化  \n",
    "    3. 最主要的原因是 : 长倚赖序列模型, 并不最适合解决情感分析问题. 这类问题主要依赖于单次出现的频率. 而这个正是全连接网络所关注的.  \n",
    "     LSTM真正的用处在于'问答'和'机器翻译'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
