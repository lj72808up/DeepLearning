{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/devkit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一. 什么是DataFlow Graph\n",
    "1. DataFlow是tensorflow的并行计算模型, 顶点代表操作, 边代表消耗或产出的tensor\n",
    "\n",
    "#### 二. 隐士创建DataFlow Graph\n",
    "1. 所有创建的tensor都会被自动加入默认的隐式创建的graph. 即以下tensorflow的api, 会同时创建图的顶点(操作)和边(输出张量)  \n",
    "  1. `tf.constant(42.0)` : 创建单一操作`tf.Operation`,产生标量42.0,并加入默认图\n",
    "  3. `tf.matmul(x, y)` : 创建单一操作`tf.Operation`, 计算x和y的矩阵乘组作为输出张量, 并加入默认图\n",
    "  4. `tf.Variable(0)` : 创建操作`tf.Operation`,存储一个可写张量, 在调用`tf.Session.run`之前.`tf.Variable` 对象有 `assign` 和`assign_add`方法创建`tf.Operation`对象(运算).\n",
    "  5. `tf.train.Optimizer.minimize` : 增加`tf.Operation`和`tf.Tensor`到默认图, 并计算其梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三. 命名空间\n",
    "1. 每个操作都有别名, 可以再声明tensor时指定相应操作的别名  \n",
    "```python\n",
    "tf.constant(0, name=\"c\")\n",
    "```\n",
    "2. 使用`tf.name_scope(\"ns\")`指定命名空间, 这里面定义的操作命名都有该命名空间的前缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_0: Tensor(\"c:0\", shape=(), dtype=int32)\n",
      "c_1: Tensor(\"c_1:0\", shape=(), dtype=int32)\n",
      "c_2: Tensor(\"outer/c:0\", shape=(), dtype=int32)\n",
      "c_3: Tensor(\"outer/inner/c:0\", shape=(), dtype=int32)\n",
      "c_4: Tensor(\"outer/c_1:0\", shape=(), dtype=int32)\n",
      "c_5: Tensor(\"outer/inner_1/c:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "c_0 = tf.constant(0, name=\"c\")  # => operation named \"c\"\n",
    "print('c_0:',c_0)\n",
    "# Already-used names will be \"uniquified\".\n",
    "c_1 = tf.constant(2, name=\"c\")  # => operation named \"c_1\"\n",
    "print('c_1:',c_1)\n",
    "\n",
    "# Name scopes add a prefix to all operations created in the same context.\n",
    "with tf.name_scope(\"outer\"):\n",
    "    c_2 = tf.constant(2, name=\"c\")  # => operation named \"outer/c\"\n",
    "    print('c_2:',c_2)\n",
    "\n",
    "    # Name scopes nest like paths in a hierarchical file system.\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_3 = tf.constant(3, name=\"c\")  # => operation named \"outer/inner/c\"\n",
    "        print('c_3:',c_3)\n",
    "\n",
    "    # Exiting a name scope context will return to the previous prefix.\n",
    "    c_4 = tf.constant(4, name=\"c\")  # => operation named \"outer/c_1\"\n",
    "    print('c_4:',c_4)\n",
    "\n",
    "    # Already-used name scopes will be \"uniquified\".\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_5 = tf.constant(5, name=\"c\")  # => operation named \"outer/inner_1/c\"\n",
    "        print('c_5:',c_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四. 执行图中的操作\n",
    "1. 使用dict结构, 指定张量所依赖的placeholder张量\n",
    "2. `sess.run()`中增加metadata参数, 来把计算过程存储到一个容器中, 以便回溯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[3]) #一维向量\n",
    "y = tf.square(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y,{x:[1,2,3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul([[37.0, -23.0], [1.0, 4.0]], tf.random_uniform([2, 2]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Define options for the `sess.run()` call.\n",
    "#     options = tf.RunOptions()\n",
    "#     options.output_partition_graphs = True\n",
    "#     options.trace_level = tf.RunOptions.FULL_TRACE\n",
    "\n",
    "    # Define a container for the returned metadata.\n",
    "#     metadata = tf.RunMetadata()\n",
    "\n",
    "#     sess.run(y, options=options, run_metadata=metadata)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(y))\n",
    "\n",
    "    # Print the subgraphs that executed on each device.\n",
    "#     print(metadata.partition_graphs)\n",
    "\n",
    "    # Print the timings of each operation that executed.\n",
    "#     print(metadata.step_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 五. 可视化计算图\n",
    "`tf.summary.FileWriter(\"/tmp/log/testtf\", sess.graph)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your graph.\n",
    "x = tf.constant([[37.0, -23.0], [1.0, 4.0]])\n",
    "w = tf.Variable(tf.random_uniform([2, 2]))\n",
    "y = tf.matmul(x, w)\n",
    "# ...\n",
    "loss = ...\n",
    "train_op = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # `sess.graph` provides access to the graph used in a <a href=\"../api_docs/python/tf/Session\"><code>tf.Session</code></a>.\n",
    "    writer = tf.summary.FileWriter(\"/tmp/log/testtf\", sess.graph)\n",
    "\n",
    "    # Perform your computation...\n",
    "    for i in range(1000):\n",
    "    sess.run(train_op)\n",
    "    # ...\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
