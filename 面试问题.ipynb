{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA相关\n",
    "#### 1. LDA的文档生成模型: \n",
    "  * 先选择一个主题,再从这个主题中选择一个一个单词, 形成文章的第1个单词; \n",
    "  \t再重新选择一个主题, 在选择一个单词, 形成文章的第二个单词.\n",
    "  * 而k个主题的分布式二项分布, 所以选择狄利克分布作为topic分布的先验分布\n",
    "  \t且每个主题下的单词分布也是二项分布, 同样选择狄利克雷分布作为其先验分布\n",
    "\n",
    "#### 2. 确定主题部分和主题下词分布的过程: \n",
    "  * 首先确定2个狄利克雷分布各自的伪计数, 将其作为作为先验, 然后为语料库中每个文档的每个单词随机设置一个topic编号, 在利用Gibbs采样更新每个词的topic编号, 直到收敛. 此时得到了稳定的每篇文章的单词的主题标号.  \n",
    "  * 然后统计topic维度的概率, \n",
    "      统计每个topic下词的概率\n",
    "\n",
    "#### 3. 如何预测新文章的主体分布\n",
    "  * 首先为新文档的每个单词随机分配一个主题偏好\n",
    "  * 然后重新扫描文档, 用Gibbs采样更新它的topic, 直到采样收敛\n",
    "  * 最后, 拥挤文档中每个单词的主题, 得到这个文档的主体分布\n",
    "\n",
    "### PCA相关\n",
    "1. PCA的目标是找到一个坐标投影, 把n维特征投影到小于n的坐标上\n",
    "2. PCA的每一个特征都是原来特征的线性组合\n",
    "3. 前几个主成分表示了绝大部分的数据方差\n",
    "\n",
    "### EM相关\n",
    "1. 确定一个多项分布, 表示每个类簇的分布.  \n",
    "2. 设每个类簇都是多元高斯分布, 因此, 要随机赋予2个高斯分布的均值和方差\n",
    "3. 开始迭代执行算法\n",
    "  * E步: 计算每个样本, 再分别的高斯分布下得到的概率\n",
    "  * M步: 用上面的概率, 加权计算每个高斯分布的均值和方差\n",
    "4. 迭代上述算法直到收敛\n",
    "5. 可见, GMM对均值和方差的初始选择很敏感, 因此, 可先使用KMeans给出一个硬分类下的均值和方差, 再执行EM迭代\n",
    "\n",
    "\n",
    "\n",
    "### BPR相关\n",
    "\n",
    "\n",
    "### 深宽相关"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
