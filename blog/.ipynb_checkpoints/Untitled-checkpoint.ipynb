{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络与逻辑回归\n",
    "\n",
    "1. 神经网络为何会流行起来?\n",
    "训练样本的规模越超大,预测效果越好,这时传统机器学习方法抵不上的\n",
    "![avatar](../img/dl1.png)\n",
    "\n",
    "## 1.1 浅层神经网络\n",
    "1. 两层神经网络 (单隐层神经网络)  \n",
    "( 1 )神经网络的中间节点被称作”隐藏层”.   \n",
    "( 2 )层数计算不包括输入的特征属性层, 但包括输出层  \n",
    "( 3 )每层神经网络的计算结果,用${ a }_{ j }^{ [i] }$表示, 其中$i$表示当前处在第几层网络中 (输入特征层算第0层)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;下标$j$表示该层的第几个节点输出的结果. $a$表示active激活函数  \n",
    "![avatar](../img/lesslayer.png)\n",
    "\n",
    "## 1.2 深层神经网络\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 超参数调试,正则化及优化\n",
    "### 一. 深度学习的使用层面\n",
    "1. 训练,开发(验证),测试集的分配比例  \n",
    "( 1 )小规模数据量情况下: 70% 20% 10%  \n",
    "( 2 )大数据下: 98% 1% 1%  \n",
    "( 3 )保证验证集合测试集来自同一个数据集, 这样才能对分布进行无偏估计  \n",
    "2. 偏差与方差    \n",
    "( 1 )偏差 : 训练集上的表现  \n",
    "( 2 )方差: 验证集上的表现  \n",
    "( 3 )高偏差合高方差时的选择  \n",
    "![avatar](../img/1-highbiasandvariance.png)\n",
    "\n",
    "### 二. 正则化\n",
    "1. L2正则化  \n",
    "带正则化的神经网络优化函数: $J({ w }^{ [1] },{ b }^{ [1] },{ w }^{ [2] },{ b }^{ [2] }...{ w }^{ [L] },{ b }^{ [L] })=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ L(\\widehat { { y }^{ (i) } } ,{ y }^{ (i) }) } +\\frac { \\lambda  }{ 2m } \\sum _{ l=1 }^{ L }{ { \\parallel W\\parallel  }_{ F }^{ 2 } } $  \n",
    "其中, ${ \\parallel W\\parallel  }_{ F }^{ 2 }$称作Frobnius范数, 角标$F$代表Frobnius  \n",
    "2. 数据扩增正则化  \n",
    "很多情况下, 我们的样本不足. 比如计算机视觉里需要大量的输入样本进行学习(因为图片上的像素点有很多). 此时我们通过裁剪图片, 扭曲图片, 翻转图片来扩大测试集. 由于这些扩大的样本实在已有样本基础上加工而来, 而不是全新的训练样本, 因此可以起到正则化的作用\n",
    "![avatar](../img/1-expendimg.png)  \n",
    "3. Early Stoping  \n",
    "Early Stoping意为提早结束算法学习. 每次完成一轮梯度下降后, 记录该算法在测试集和训练集上损失函数的值, 绘制曲线后发现, 通常训练集上的损失函数值会一路下降, 验证集上的函数值呈现先下降后上升的情况, 在验证集上损失函数值最低处停止学习. \n",
    "\n",
    "### 三.Dropout随机失活  \n",
    "1. 随机失活的做法:  \n",
    "( 1 )遍历神经网络的每一层, 并设置神经网络中每个节点消失的概率.(每个节点都以抛硬币的方式设置概率, 因此每个节点得以保留的概率为0.5), 此后我们随机删除一些节点, 并删除从该节点进出的连线, 最后得到一个节点更少, 规模更小的神经网络. 反向传播时, 就在这个被简化的网络上进行. 对于不同的训练样本, 可以重新选择失活的节点  \n",
    "( 2 )随机失活可有效解决过拟合问题, 因为每个样本的训练, 都是在训练一个小得多的网络  \n",
    "2. 反向随机失活 - inverted dropout  \n",
    "假设在第3层上进行随机失活, 这里只演示了前向传播中的dropout, 反向传播同理  \n",
    "( 1 ) keep_prob:节点得以保存的概率  \n",
    "( 2 ) d3:第三层的dropout向量  \n",
    "( 3 ) a3:第三层隐藏单元的激活函数, 经编码后, a3向量中为0的对应该节点已被dropout   \n",
    "```python\n",
    "# 设置每个节点得以保留的概率最多为0.8\n",
    "keepprob = 0.8\n",
    "# 生成一个与a3相同规模的dropout失活矩阵\n",
    "d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keepprob\n",
    "# 还是原来一样计算正向传播,只是在结果上乘以节点存活的概率, 来模拟节点失活\n",
    "a3 = np.multiply(a3,d3)\n",
    "# 由于我们只保留了80%的节点,因此最终结果还要除以80%,保证a3的结果不变\n",
    "a3 = a3/0.8\n",
    "```\n",
    "3. Dropout的直观理解  \n",
    "( 1 )对每个隐藏单元来讲, 他们的每个输入都可能会随机消失. 直观上好像在说, 因为这些输入可能随机删除, 我们就不会对某个输入加上过大的权重(W向量), 如此来处理过拟合  \n",
    "( 2 )每层的keep_prob可以设置为不同值.如下图, 第2层隐藏层的系数矩阵${ W }^{ [2] }\\in { R }^{ 7*7 }$, 该层产生过拟合的概率大, 则可以把该层的keep_prob设置小一些.keep_prob=1表示保留所有单元 (通常输入层和输出层的keep_prob=1)  \n",
    "![avatar](../img/1-dropout.png) \n",
    "( 3 )通常情况, 都会先关闭dropout, 即把每层的keep_prob设置为1. 在确保每次迭代,cost function都在下降, 即代码正确运行后, 再打开dropout进行正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加速神经网络训练\n",
    "### 一. 正则化输入\n",
    "1. 属性正则化的做法  \n",
    "更新样本的feature形式, $x=\\frac { x-\\mu  }{ { \\sigma  }^{ 2 } } $,其中  \n",
    "( 1 ) ${ { \\mu  } }_{ i }=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { x }_{ i } } $  \n",
    "( 2 ) ${ \\sigma  }_{ i }^{ 2 }=\\sum _{ i=1 }^{ m }{ { { (x }_{ i } }-{ \\mu  }_{ i })^{ 2 } } $    \n",
    "2. 为什么正则化输入可以加快训练速度  \n",
    "正则化输入, 使得所有属性的均值为0, 方差为1. 把所有属性正则化后, 使得代价函数变得更为对称(椭球变成球), 梯度下降时, 就可以选择稍大的学习率, 增加每次梯度下降的步长, 加速学习\n",
    "\n",
    "### 二. 梯度消失与梯度爆炸\n",
    "1. 梯度爆炸与梯度消失是如何发生的  \n",
    "设有一个超多层的神经网络如下图所示. 为简化训练过程的描述, 假设每个隐藏层的激活函数g(z)=z,且b=0\n",
    "![avatar](../img/1-tiduxiaoshi.png)\n",
    " 前者导致反向传播时产生的梯度值很大, 即梯度爆炸. 后者导致梯度值很小, 接近于0, 造成梯度消失\n",
    "2. 权重初始化选择  \n",
    "我们从单层神经网络开始, 来探究不同激活函数下, 参数的选择方法. (多层神经网络的输入x变为${ a }^{ l-1 }$)  \n",
    "![avatar](../img/1-singlenet.png)\n",
    "( 1 ) sigmoid函数, 则选择系数$\\frac { 1 }{ n } \\Longrightarrow  { W }^{ [l] } = np.random.randn(shape)*np.sqrt(1.0/n[L-1])$   \n",
    "( 2 ) Relu函数, 则选择系数$\\frac { 2 }{ n } \\Longrightarrow  { W }^{ [l] } = np.random.randn(shape)*np.sqrt(2.0/n[L-1])$  \n",
    "( 3 ) tanh函数, 则选择系数$\\frac { 1 }{ n }或\\frac { 1 }{ { n }^{ l }*{ n }^{ l-1 } }$\n",
    "\n",
    "### 三. 使用mini-batch梯度下降\n",
    "1. 为什么使用mini-batch  \n",
    "传统梯度下降要计算所有样本的一个下降值, 然后取其平均.但神经网络往往有巨大的训练集, 所以不能让梯度下降一次计算全部样本, 转而选择一个子集在还是那个面进行梯度下降\n",
    "```python\n",
    "m = 100000\n",
    "batch = 1000\n",
    "Repeat until convege{\n",
    "    for i in range(0,m/batch):\n",
    "        x_batch = x.iloc[:,i*batch:(i+1)*batch]\n",
    "        y_batch = x.iloc[:,i*batch:(i+1)*batch]\n",
    "        # 梯度下降\n",
    "}\n",
    "```\n",
    "2. 典型的batch步长选择\n",
    "通常选择${ 2 }^{ n }$作为batch的步长 , $eg:64,128,512,1024 ...$\n",
    "\n",
    "### 四. 指数加权平均\n",
    "统计中称作指数加权移动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 100000\n",
    "batch = 1000\n",
    "Repeat until convege{\n",
    "    for i in range(0,m/batch):\n",
    "        x_batch = x.iloc[:,i*batch:(i+1)*batch]\n",
    "        y_batch = x.iloc[:,i*batch:(i+1)*batch]\n",
    "        # 梯度下降\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
