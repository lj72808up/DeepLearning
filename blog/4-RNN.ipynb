{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN序列模型\n",
    "-----------\n",
    "## 一. RNN基本概念\n",
    "#### 1. 适用场景  \n",
    "( 1 ) 语音识别: $x$: 音频录音, $y$: 输出一段文字记录  \n",
    "( 2 ) 音乐生成: $y$: 一段某种风格的音乐  \n",
    "( 3 ) DNA序列分析: 判断某段DNA序列是否符合某种蛋白质  \n",
    "( 4 ) 机器翻译: $x$: 某种语言的文字, $y$: 另一种语言的文字\n",
    "\n",
    "#### 2. 数学模型  \n",
    "1. 符号表示  \n",
    "假设我们要判断一个句子中的单词, 是否是人名. 我们的输入$x$是一个句子, 输出$y$是一个向量, 每个元素表示居中对应位置的单词是否是人名  \n",
    "$X$ : Harry Potter and Hermione Granger invetened a new spell.  \n",
    "( 1 ) 使用${x}^{<t>}$表示举止中第$t$个单词. 上句为${x}^{<1>}...{x}^{<9>}$  \n",
    "$y$ : 1      1      0      1      1       0      0   0     0  \n",
    "( 2 ) 使用${y}^{<t>}$表示句子中第$t$个单词是否是人名. ${T}_{<x>}$和${T}_{<y>}$表示输入序列长度和输出序列长度  \n",
    "( 3 ) ${x}^{(i)<t>}$ : 第$i$个样本输入的第$t$个序列  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;${y}^{(i)<t>}$ : 第$i$个样本输出的第$t$个序列  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;${T}^{(i)}_{x}$ : 第$i$个样本的输入序列长度  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;${T}^{(i)}_{y}$ : 第$i$个样本的输出序列长度  \n",
    "\n",
    "2. 如何表示句中的每个单词    \n",
    "每个单词都表示成一个向量. 长度为词典长度\n",
    "\n",
    "<img src=\"../img/rnn1.png\" width=\"60%\" height=\"60%\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 二. RNN的正反传播\n",
    "#### 1. RNN正向传播\n",
    "1. 传统神经网络的弊端  \n",
    "( 1 ) 不同样本的输入输出的长度${T}^{<x>}$和${T}^{<y>}$会不同  \n",
    "( 2 ) 文章不同位置的特征之间不能共享学习到的信息  \n",
    "( 3 ) 如果每个隐藏单元的参数, 都是样本中最大单词数量*10000维, 那这个参数矩阵会很庞大  \n",
    "<img src=\"../img/3-rnn2.png\"  width=\"35%\" height=\"35%\"/>   \n",
    "2. RNN将不同时间片训练的结果同样加入训练过程, 即$y^{i<3>}$不只依靠$x^{i<3>}$, 还依靠$x^{i<1>}$与$x^{i<2>}$. 其正向传播过程如下:  \n",
    "<img src=\"../img/3-rnnforword.png\"  width=\"60%\" height=\"60%\"/>  \n",
    "${ a }^{ <0> }=\\overrightarrow { 0 } \\quad \\quad \\\\ { a }^{ <1> }={ g }_{ 1 }\\left( { w }_{ aa }{ a }^{ <0> }+{ w }_{ ax }{ x }^{ <1> } \\right) \\Rightarrow { a }^{ <1> }的结果综合了{ a }^{ <0> }和{ x }^{ <1> }\\\\ -------------------------------------------------\\\\ { a }^{ <t> }={ g }\\left( { w }_{ aa }{ a }^{ <t-1> }+{ w }_{ ax }{ x }^{ <t> } \\right) \\xrightarrow [ 转换表示方式 ]{  } g\\left( { w }_{ a }\\left[ { a }^{ <t-1> },{ x }^{ (t) } \\right]  \\right) \\\\ { \\widehat { y }  }^{ <t> }=g\\left( { w }_{ ya }{ a }^{ <t> } \\right) ,\\\\ 其中{ w }_{ ji }是从变量i到变量j的参数;{ w }_{ a }=\\left[ { w }_{ aa }|{ w }_{ ax } \\right] $\n",
    "\n",
    "#### 2. RNN穿越时的反向传播-through time\n",
    "( 1 ) 如下图所示, 该图从下向上看, 由于RNN的每层节点, 都与时间序列有关, 因此我们把每层的$\\widehat { y } $看作当前时间点的输出, 且${ \\widehat { y }  }^{ <{ T }> }$与${a}^{<t>}$和${a}^{<t-1>}$有关  \n",
    "<img src=\"../img/rnnback.png\" width=\"45%\" height=\"45%\">  \n",
    "\n",
    "( 2 ) 损失函数:所有时间部损失的叠加 $L(\\widehat { y } ,y)=\\sum _{ t=1 }^{ { T }_{ x } }{ { L }^{ <t> }\\left( { \\widehat { y }  }^{ <t> },{ y }^{ <t> } \\right)  } $  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中: ${ L }^{ <t> }\\left( { \\widehat { y }  }^{ <t> },{ y }^{ <t> } \\right) =-{ y }^{ <t> }\\log { { \\widehat { y }  }^{ <t> } } -(1-{ y }^{ <t> })\\log { \\left( 1-{ \\widehat { y }  }^{ <t> } \\right)  } $  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上图所示, 我们的反向传播实在不同时间部上进行的, 因此也叫做穿越时间的反响传播\n",
    "\n",
    "#### 3. 其他种类的序列模型\n",
    "( 1 ) one-to-one (普通神经网络)  \n",
    "<img src=\"../img/rnn-one2one.png\" width=\"15%\" height=\"15%\">  \n",
    "( 2 ) one-to-many (音乐生成,输入1个风格,输出一个曲子(多个音符组成的序列))\n",
    "<img src=\"../img/rnnone2many.png\" width=\"25%\" height=\"25%\">  \n",
    "( 3 ) many-to-one (影评分析,输入一篇影评,输出这个作者对影片的喜好程度)  \n",
    "<img src=\"../img/rnnmany2one.png\" width=\"25%\" height=\"25%\">  \n",
    "( 4 ) many-to-many (${ T }_{ x }={ T }_{ y }$)  \n",
    "<img src=\"../img/rnnmanytomany1.png\" width=\"30%\" height=\"30%\">  \n",
    "( 5 ) many-to-many (${ T }_{ x }\\neq { T }_{ y }$,机器翻译)  \n",
    "<img src=\"../img/rnnmanytomany2.png\" width=\"30%\" height=\"30%\">  \n",
    "\n",
    "## 三. RNN构建语言模型\n",
    "#### 1. RNN构建一个语言模型\n",
    "( 1 ) 什么是语言模型  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;例如: 要识别语音,识别以下两句话说的是哪句  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sentence1: The apple and pair salad  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sentence2: The apple and pear salad  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;此时, 我们要通过构建RNN模型, 然后输出这两句话的概率. $P_1=3.2*{10}^{-13}$, $P_2=5.2*{10}^{-10}$  \n",
    "( 2 ) RNN的语言模型  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;a ) 我们的训练集来自于一个超大的语料库(corpus).语料库是由大量句子组成  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;b ) 假设要预测的语句\"Cats average 15 hours of sleep a day\". 首先要对这句话去标点, 形成只有单词组成的单词序列.把每个单词标记成词典所示的高维向量$R^{10000}$. 每个隐藏节点, 都是一个softmax节点, 输出每个单词的概率.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;c ) 如下图所示,我们先由${ x }^{ <1> }$和${ a }^{ <0> }=\\vec { 0 } $通过softmax预测出${ \\widehat { y }  }^{ <1> }$,这个${ \\widehat { y }  }^{ <1> }$只是一个预测值, 预测这句话的第一个单词是什么, 而不用去管到底是不是Cats.下一步, 我们要预测这句话的第二个单词是什么.而此时${ x }^{ <2> }$使用的是真实的第一个单词${ { y }^{ <1> } }=Cats$,然后softmax预测,就是预测的第一个但此时Cats的情况下, 第二个单词是什么.即$p(\\_|Cats)$.第三个激活单元使用${ x }^{ <3> }={ { y }^{ <2> } }$,softmax给出的概率为$p(\\_ |Cats\\quad average)$\n",
    "<img src=\"../img/rnn-anguagemodel.png\"  width=\"60%\" height=\"60%\">  \n",
    "\n",
    "#### 2. 检验语言模型  \n",
    "( 1 ) 当训练一个模型后, 要查看这个模型训练到了什么, 一种非正式方法就是进行一次新的序列采样.  \n",
    "( 2 ) 第一个节点,经过$softmax$后$a_1$输出了\\词典中每个单词在此位置出现的概率. 此时我们随机选择一个单词作为句子的第一个单词, 接下来对第二个节点采样. 选择$softmax$输出中概率最大的单词作为句子中接下来的单词  \n",
    "( 3 ) 句子的结束可以以预测到$EOS$标识时停止\n",
    "\n",
    "#### 3. 截止目前为止, 我们的语言模型的缺点\n",
    "( 1 ) 至此, 我们的语言模型并不擅长处理长倚赖问题. 例如: Cats which eat mach were full. 此句中的Cats是复数,were也要使用复数形式. 而Cats和were之间隔了很多单词, 即RNN中隔了很多时间部. 而我们现在的RNN: 下一个时间部的输入, 使用的是句子中的前一个单词向量.很难隔多个时间部反向传播到前面单元.  \n",
    "( 2 ) 深层神经网络易出现梯度消失或梯度爆炸问题, 对此, RNN的梯度爆炸可以通过梯度修剪(Gradient clipping)来处理. 如梯度超过某个预定的阈值, 就使用这个阈值进行计算."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 四. GRU-门控循环单元(Gated Recurrent Unit)\n",
    "#### 1.  GRU的改进\n",
    "GRU改变了传统RNN中隐藏层的形态, 它擅长保持时间部的深层连接, 同时解决了梯度消失问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
