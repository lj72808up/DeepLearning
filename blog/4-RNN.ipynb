{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN序列模型\n",
    "-----------\n",
    "## $\\S$1. RNN训练模型\n",
    "### 1.1 RNN基本概念\n",
    "#### 一. 适用场景  \n",
    "1. 语音识别: $x$: 音频录音, $y$: 输出一段文字记录  \n",
    "2. 音乐生成: $y$: 一段某种风格的音乐  \n",
    "3. DNA序列分析: 判断某段DNA序列是否符合某种蛋白质  \n",
    "4. 机器翻译: $x$: 某种语言的文字, $y$: 另一种语言的文字\n",
    "\n",
    "#### 二. 符号表示  \n",
    "1. 假设我们要判断一个句子中的单词, 是否是人名. 我们的输入$x$是一个句子,   \n",
    " 输出$y$是一个向量(每个元素表示句中对应位置的单词是否是人名) \n",
    "$\\begin{cases} X:\\quad Harry\\quad Potter\\quad and\\quad Hermione\\quad Granger\\quad invetened\\quad a\\quad new\\quad spell \\\\ Y:\\quad 1\\quad \\quad  \\quad 1\\quad \\quad \\quad 0\\quad \\quad \\quad \\quad 1\\quad \\quad \\quad  1\\quad \\quad \\quad \\quad 0\\quad  \\quad  0\\quad \\quad 0\\quad \\quad 0 \\end{cases}\n",
    " $\n",
    "2. 单个语句表示:   \n",
    " $\\begin{cases} { x }^{ <t> }:表示句子中第t个单词.上句为{ x }^{ <1> }...{ x }^{ <9> } \\\\ { y }^{ <t> }:表示句子中第t个单词是否是人名. \\\\ { T }_{ <x> }和{ T }_{ <y> }表示输入序列长度和输出序列长度 \\end{cases}$  \n",
    "3. 多个语句表示:   \n",
    " $\\begin{cases} { x }^{ (i)<t> }:第i个样本输入的第t个序列 \\\\ { y }^{ (i)<t> }:第i个样本输出的第t个序列 \\\\ { T }^{ (i) }_{ x }:第i个样本的输入序列长度 \\\\ { T }^{ (i) }_{ y }:第i个样本的输出序列长度 \\end{cases}$ \n",
    " \n",
    "### 1.2 RNN的正反传播\n",
    "#### 一. RNN正向传播\n",
    "1. 传统神经网络的弊端  \n",
    "  1. 不同样本的输入输出的长度${T}^{<x>}$和${T}^{<y>}$会不同  \n",
    "  2. 文章不同位置的特征之间不能共享学习到的信息  \n",
    "  3. 如果每个隐藏单元的参数, 都是样本中最大单词数量*10000维, 那这个参数矩阵会很庞大  \n",
    "<img src=\"../img/3-rnn2.png\"  width=\"35%\" height=\"35%\"/>   \n",
    "2. RNN将不同时间片训练的结果同样加入训练过程, 即$y^{i<3>}$不只依靠$x^{i<3>}$, 还依靠$x^{i<1>}$与$x^{i<2>}$. 其正向传播过程如下:  \n",
    "  1. 变量表示:$\\begin{cases} { a }^{ <0> }=\\overrightarrow { 0 }  \\\\ { a }^{ <1> }={ g }_{ 1 }\\left( { w }_{ aa }{ a }^{ <0> }+{ w }_{ ax }{ x }^{ <1> } \\right)\\Longrightarrow \\left( { a }^{ <1> }的结果综合了{ a }^{ <0> }和{ x }^{ <1> },g\\left( x \\right) 为sigmoid函数 \\right)  \\\\ 即{ a }^{ <t> }={ g }\\left( { w }_{ aa }{ a }^{ <t-1> }+{ w }_{ ax }{ x }^{ <t> } \\right) \\Longrightarrow g\\left( { w }_{ a }\\left[ { a }^{ <t-1> },{ x }^{ (t) } \\right]  \\right) ,其中{ w }_{ ji }是从变量i到变量j的参数;{ w }_{ a }=\\left[ { w }_{ aa }|{ w }_{ ax } \\right]  \\\\ { \\widehat { y }  }^{ <t> }=g\\left( { w }_{ ya }{ a }^{ <t> } \\right)  \\end{cases}$\n",
    "  2. 传播过程:  \n",
    "   <img src=\"../img/3-rnnforword.png\"  width=\"60%\" height=\"60%\"/>  \n",
    "\n",
    "#### 二. RNN穿越时的反向传播-through time\n",
    "1. 如下图所示, 该图从下向上看, 由于RNN的每层节点, 都与时间序列有关, 因此我们把每层的$\\widehat { y } $看作当前时间点的输出, 且${ \\widehat { y }  }^{ <{ T }> }$与${a}^{<t>}$和${a}^{<t-1>}$有关  \n",
    "<img src=\"../img/rnnback.png\" width=\"45%\" height=\"45%\">  \n",
    "2. 损失函数:  \n",
    " 所有时间部损失的叠加 $L(\\widehat { y } ,y)=\\sum _{ t=1 }^{ { T }_{ x } }{ { L }^{ <t> }\\left( { \\widehat { y }  }^{ <t> },{ y }^{ <t> } \\right)  } $  \n",
    "  1. 其中: ${ L }^{ <t> }\\left( { \\widehat { y }  }^{ <t> },{ y }^{ <t> } \\right) =-{ y }^{ <t> }\\log { { \\widehat { y }  }^{ <t> } } -(1-{ y }^{ <t> })\\log { \\left( 1-{ \\widehat { y }  }^{ <t> } \\right)  } $  \n",
    "  2. 如上图所示, 我们的反向传播实在不同时间部上进行的, 因此也叫做穿越时间的反响传播\n",
    "\n",
    "#### 三. 其他种类的序列模型\n",
    "1. one-to-one (普通神经网络)  \n",
    "<img src=\"../img/rnn-one2one.png\" width=\"15%\" height=\"15%\">  \n",
    "2. one-to-many (音乐生成,输入1个风格,输出一个曲子(多个音符组成的序列))\n",
    "<img src=\"../img/rnnone2many.png\" width=\"25%\" height=\"25%\">  \n",
    "3. many-to-one (影评分析,输入一篇影评,输出这个作者对影片的喜好程度)  \n",
    "<img src=\"../img/rnnmany2one.png\" width=\"25%\" height=\"25%\">  \n",
    "4. many-to-many (${ T }_{ x }={ T }_{ y }$)  \n",
    "<img src=\"../img/rnnmanytomany1.png\" width=\"30%\" height=\"30%\">  \n",
    "5. many-to-many (${ T }_{ x }\\neq { T }_{ y }$,机器翻译)  \n",
    "<img src=\"../img/rnnmanytomany2.png\" width=\"30%\" height=\"30%\">  \n",
    "\n",
    "### 1.3 RNN构建语言模型\n",
    "#### 一. RNN构建一个语言模型\n",
    "1. 什么是语言模型  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;例如: 要识别语音,识别以下两句话说的是哪句  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sentence1: The apple and pair salad  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sentence2: The apple and pear salad  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;此时, 我们要通过构建RNN模型, 然后输出这两句话的概率. $P_1=3.2*{10}^{-13}$, $P_2=5.2*{10}^{-10}$  \n",
    "2. RNN的语言模型  \n",
    "  1. 我们的训练集来自于一个超大的语料库(corpus).语料库是由大量句子组成  \n",
    "  2. 假设要预测的语句\"Cats average 15 hours of sleep a day\". 首先要对这句话去标点, 形成只有单词组成的单词序列.把每个单词标记成词典所示的高维向量$R^{10000}$. 每个隐藏节点, 都是一个softmax节点, 输出每个单词的概率.  \n",
    "  3. 如下图所示,我们先由${ x }^{ <1> }$和${ a }^{ <0> }=\\vec { 0 } $通过softmax预测出${ \\widehat { y }  }^{ <1> }$,这个${ \\widehat { y }  }^{ <1> }$只是一个预测值, 预测这句话的第一个单词是什么, 而不用去管到底是不是Cats.下一步, 我们要预测这句话的第二个单词是什么.而此时${ x }^{ <2> }$使用的是真实的第一个单词${ { y }^{ <1> } }=Cats$,然后softmax预测,就是预测的第一个词是Cats的情况下, 第二个单词是什么.即$p(\\_|Cats)$.第三个激活单元使用${ x }^{ <3> }={ { y }^{ <2> } }$,softmax给出的概率为$p(\\_ |Cats\\quad average)$\n",
    "<img src=\"../img/rnn-anguagemodel.png\"  width=\"60%\" height=\"60%\">  \n",
    "\n",
    "#### 二. 检验语言模型  \n",
    "1. 当训练一个模型后, 要查看这个模型训练到了什么, 一种非正式方法就是进行一次新的序列采样.  \n",
    "2. 第一个节点,经过$softmax$后$a_1$输出了\\词典中每个单词在此位置出现的概率. 此时我们随机选择一个单词作为句子的第一个单词, 接下来对第二个节点采样. 选择$softmax$输出中概率最大的单词作为句子中接下来的单词  \n",
    "3. 句子的结束可以以预测到$EOS$标识时停止\n",
    "\n",
    "#### 三. 截止目前为止, 我们的语言模型的缺点\n",
    "1. 至此, 我们的语言模型并不擅长处理长倚赖问题. 例如: Cats which eat mach were full. 此句中的Cats是复数,were也要使用复数形式. 而Cats和were之间隔了很多单词, 即RNN中隔了很多时间部. 而我们现在的RNN: 下一个时间部的输入, 使用的是句子中的前一个单词向量.很难隔多个时间部反向传播到前面单元.  \n",
    "2. 深层神经网络易出现梯度消失或梯度爆炸问题, 对此, RNN的梯度爆炸可以通过梯度修剪(Gradient clipping)来处理. 如梯度超过某个预定的阈值, 就使用这个阈值进行计算."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $\\S$2. 几种RNN网络模型\n",
    "### 2.1 GRU-门控循环单元(Gated Recurrent Unit)\n",
    "#### 一.  GRU的改进\n",
    "1. GRU改变了传统RNN中隐藏层的形态, 它擅长保持时间部之间非常非常深层的连接, 同时解决了梯度消失问题  \n",
    "2. 传统RNN单元可视化\n",
    "<img src=\"../img/rnn-unit.png\">\n",
    "\n",
    "#### 二. 简化GRU模型\n",
    "1. 当我们从左到右读句子\"Cats which eat mach were full\", 需要记住Cat是单数还是复数.在GRU中,增加组件记忆单元\"c\",来记住这些信息.   \n",
    "GRU中的t个时间部的${ c }^{ <t> }={ x }^{ <t> }$.$Gated$的意思代表, 我们设置一个\"门\", 来决定更新记忆单元的比例.过程如下:  \n",
    "  1. 首先计算该单元的${ \\widetilde { c }  }^{ <t> }=tanh\\left( { w }_{ c }\\left[ { c }^{ <t-1> },{ x }^{ <t> } \\right]  \\right)$   \n",
    "  2. 使用$sigmoid$函数计算更新门${ \\Gamma  }_{ u }=\\delta \\left( { w }_{ u }\\left[ { c }^{ <t-1> },{ x }^{ <t> } \\right]  \\right) $   \n",
    "  3. 计算加上门值的记忆单元 ${ c }^{ <t> }={ \\Gamma  }_{ u }*{ \\widetilde { c }  }^{ <t> }+\\left( 1-{ \\Gamma  }_{ u } \\right) *{ c }^{ <t-1> }$  \n",
    "若${ \\Gamma  }_{ u }=0$, 则不去更新记忆单元的值, 即${ c }^{ <t> }={ c }^{ <t-1> }$  \n",
    "2. 为什么加上更新门的GRU单元可以维持长久记忆, 避免梯度消失\n",
    "因为更新门${ \\Gamma  }_{ u }$的数值往往很小, 很接近与0. 因此使得${ c }^{ <t> }$很接近${ c }^{ <t-1> }$ . 所以能够记住之前单元的数值. 而且由于数值相近, 即使神经网络层数越多, 其梯度也不会产生小时消失问题 \n",
    "\n",
    "#### 三. 完整的GRU单元\n",
    "1. GRU和LSTM是两个具体化的RNN实例. 他们给出了RNN单元的计算方法. 完整的GRU计算过程如下   \n",
    "2. $\\begin{cases} (1) { \\Gamma  }_{ r }=\\delta \\left( { w }_{ r }\\left[ { c }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ r } \\right)  \\\\ (2) { \\widetilde { c }  }^{ <t> }=tanh\\left( { w }_{ c }\\left[ { { \\Gamma  }_{ r }*c }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ c } \\right)  \\\\ (3) { \\Gamma  }_{ u }=\\delta \\left( { w }_{ u }\\left[ { c }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ u } \\right)  \\\\ (4) { c }^{ <t> }={ \\Gamma  }_{ u }*{ \\widetilde { c }  }^{ <t> }+\\left( 1-{ \\Gamma  }_{ u } \\right) *{ c }^{ <t-1> } \\end{cases}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.2 LSTM-长短期记忆(long short term memory)\n",
    "#### 一. 基本概念\n",
    "1. LSTM是比GRU更大更常用的RNN网络. 他在梯度消失等细节问题上展开了开创性的讨论  \n",
    " GRU简化了LSTM模型, 完整的GRU只有两个门${ \\Gamma  }_{ r }$和${ \\Gamma  }_{ u }$. 而LSTM有三个门\n",
    "2. 计算公式: \n",
    "${ \\widetilde { c }  }^{ <t> }=tanh\\left( { w }_{ c }\\left[ { a }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ c } \\right) \\\\ { \\Gamma  }_{ u }=\\delta \\left( { w }_{ u }\\left[ { a }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ u } \\right) \\quad 更新门\\\\ { \\Gamma  }_{ f }=\\delta \\left( { w }_{ f }\\left[ { a }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ f } \\right) \\quad 遗忘门\\\\ { \\Gamma  }_{ o }=\\delta \\left( { w }_{ o }\\left[ { a }^{ <t-1> },{ x }^{ <t> } \\right] +{ b }_{ o } \\right) \\\\ { c }^{ <t> }={ \\Gamma  }_{ u }*{ \\widetilde { c }  }^{ <t> }+{ \\Gamma  }_{ f }*{ c }^{ <t-1> }\\\\ { a }^{ <t> }={ \\Gamma  }_{ o }*{ c }^{ <t> }$\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 循环网络\n",
    "#### 一. 基本概念\n",
    "1. 双向循环网络-BRNN(Bidirectional RNN)\n",
    "双向RNN会考虑该时间部之前和之后的时间部输出. 例如在识别下列句中的\"Teddy\"是否是人名, 不仅需要看\"Teddy\"后面跟了什么语句, 还要看\"Teddy\"前面是什么语句. 此时就需要双向RNN网络. 双向RNN的每个单元可以是GRU或LSTM\n",
    "2. 如下图, 每个神经单元, 都要处理两个方向的$a$,$\\overrightarrow { a }$与$\\overleftarrow { a } $\n",
    "<img src=\"../img/BRNN.png\" width=\"58%\" height=\"58%\">\n",
    "\n",
    "### 2.4 深层循环神经网络\n",
    "#### 一. 基本概念\n",
    "1. 以上的介绍的RNN都是只有一层的RNN,只是时间部串联起来的水平方向的单层神经网络. \n",
    " 深度循环神经网络是指, 在时间部这个维度上, 再增加一个隐藏层的维度.  \n",
    "2. 深层循环神经网络的隐藏层是二维的, 时间部和层数. 因此, 当前隐藏层的输入为\"前一层同时间部的节点\"和\"本层上一时间部的节点\". \n",
    " 我们用${ a }^{ [i]<j> }$表示第$i$层第$j$个时间部的处理单元, 因此有${ a }^{ [2]<3> }=g\\left( { w }_{ a }^{ [2] }\\left[ { a }^{ [2][2] },{ a }^{ [1]<3> } \\right]  \\right) $\n",
    "3. 如下图所示, 横向为时间部的展开, 总想为隐藏层的展开\n",
    " <img src=\"../img/deeeprnn.png\" width=\"60%\" height=\"60%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
