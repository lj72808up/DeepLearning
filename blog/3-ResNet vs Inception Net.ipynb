{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ResNet\n",
    "#### 一. ResNet\n",
    "1. Residual Block - 残差块    \n",
    "  1. 参差的意思为, 由于深度神经网络中存在梯度消失或梯度爆炸现象. 在计算${ a }^{ [l+2] }=g({ z }^{ [l+2] })$时往往得不到正确结果    \n",
    "  2. 此时计算${ a }^{ [l+2] }=g({ z }^{ [l+2] }+{ a }^{ [l] })$的方法, 加入残差项${ a }^{ [l] }$    \n",
    "<img src='../img/residualblock1.png' height='80%' width='80%'>\n",
    "\n",
    "2. Residual Network  \n",
    " 参差网络为使用残差块构成的网络\n",
    " <img src='../img/residualnetwork.png' height='70%' width='70%'>\n",
    "\n",
    "####  二. ResNet的良好表现\n",
    "1. ResNet的表现  \n",
    "  1. ResNet可以训练很深的神经网络 (往往达到100多层), 而避免梯度消失和梯度爆炸问题\n",
    "  2. ResNet随着网络层数的增多, 其有更良好的表现. 而不会像传统网络一样, 网络层数增多到一定程度带来负面效果\n",
    "3. 为什么参差网络可以构建超深的网络  \n",
    " 由于${ a }^{ [l+2] }=g({ z }^{ [l+2] }+{ a }^{ [l] })$, 因此即使"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inception network\n",
    "\n",
    "#### 一. 1\\*1 filter矩阵  \n",
    "1. 1\\*1 宽度和高度的filter矩阵, 可以保持输出矩阵和输入矩阵有相同的宽度和高度  \n",
    "2.  根据filter个数的不同, 1\\*1\\*n的filter可以减小输入矩阵的#channels数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
